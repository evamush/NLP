{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\bar}{\\,|\\,}\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\weights}{\\mathbf{w}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the last assignment, you will apply deep learning methods to solve a particular story understanding problem. Automatic understanding of stories is an important task in natural language understanding [[1]](http://anthology.aclweb.org/D/D13/D13-1020.pdf). Specifically, you will develop a model that given a sequence of sentences learns to sort these sentence in order to yield a coherent story [[2]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/short-commonsense-stories.pdf). This sounds (and to an extent is) trivial for humans, however it is a quite difficult task for machines as it involves commonsense knowledge and temporal understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "You are given a dataset of 45502 instances, each consisting of 5 sentences. Your system needs to ouput a sequence of numbers which represent the predicted order of these sentences. For example, given a story:\n",
    "\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp.\n",
    "    Jan decided to get a new lamp.\n",
    "    Jan's lamp broke.\n",
    "\n",
    "your system needs to provide an answer in the following form:\n",
    "\n",
    "    2\t3\t4\t1\t0\n",
    "\n",
    "where the numbers correspond to the zero-based index of each sentence in the correctly ordered story. So \"`2`\" for \"`He went to the store.`\" means that this sentence should come 3rd in the correctly ordered target story. In This particular example, this order of indices corresponds to the following target story:\n",
    "\n",
    "    Jan's lamp broke.\n",
    "    Jan decided to get a new lamp.\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "To develop your model(s), we provide a training and a development datasets. The test dataset will be held out, and we will use it to evaluate your models. The test set is coming from the same task distribution, and you don't need to expect drastic changes in it.\n",
    "\n",
    "You will use [TensorFlow](https://www.tensorflow.org/) to build a deep learning model for the task. We provide a very crude system which solves the task with a low accuracy, and a set of additional functions you will have to use to save and load the model you create so that we can run it.\n",
    "\n",
    "As we have to run the notebooks of each submission, and as deep learning models take long time to train, your notebook **NEEDS** to conform to the following requirements:\n",
    "* You **NEED** to run your parameter optimisation offline, and provide your final model saved by using the provided function\n",
    "* The maximum size of a zip file you can upload to moodle is 160MB. We will **NOT** allow submissions larger than that.\n",
    "* We do not have time to train your models from scratch! You **NEED** to provide the full code you used for the training of your model, but by all means you **CANNOT** call the training method in the notebook you will send to us.\n",
    "* We will run these notebooks automatically. If your notebook runs the training procedure, in addition to loading the model, and we need to edit your code to stop the training, you will be penalised with **-20 points**.\n",
    "* If you do not provide a pretrained model, and rely on training your model on our machines, you will get **0 points**.\n",
    "* It needs to be tested on the stat-nlp-book Docker setup to ensure that it does not have any dependencies outside of those that we provide. If your submission fails to adhere to this requirement, you will get **0 points**.\n",
    "\n",
    "Running time and memory issues:\n",
    "* We have tested a possible solution on a mid-2014 MacBook Pro, and a few epochs of the model run in less than 3min. Thus it is possible to train a model on the data in reasonable time. However, be aware that you will need to run these models many times over, for a larger number of epochs (more elaborate models, trained on much larger datasets can train for weeks! However, this shouldn't be the case here.). If you find training times too long for your development cycle you can reduce the training set size. Once you have found a good solution you can increase the size again. Caveat: model parameters tuned on a smaller dataset may not be optimal for a larger training set.\n",
    "* In addition to this, as your submission is capped by size, feel free to experiment with different model sizes, numeric values of different precisions, filtering the vocabulary size, downscaling some vectors, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "A non-exhaustive list of things you might want to give a try:\n",
    "- better tokenization\n",
    "- experiment with pre-trained word representations such as [word2vec](https://code.google.com/archive/p/word2vec/), or [GloVe](http://nlp.stanford.edu/projects/glove/). Be aware that these representations might take a lot of parameters in your model. Be sure you use only the words you expect in the training/dev set and account for OOV words. When saving the model parameters, pre-rained word embeddings can simply be used in the word embedding matrix of your model. As said, make sure that this word embedding matrix does not contain all of word2vec or GloVe. Your submission is limited, and we will not allow uploading nor using the whole representations set (up to 3GB!)\n",
    "- reduced sizes of word representations\n",
    "- bucketing and batching (our implementation is deliberately not a good one!)\n",
    "  - make sure to draw random batches from the data! (we do not provide this in our code!)\n",
    "- better models:\n",
    "  - stacked RNNs (see tf.nn.rnn_cell.MultiRNNCel\n",
    "  - bi-directional RNNs\n",
    "  - attention\n",
    "  - word-by-word attention\n",
    "  - conditional encoding\n",
    "  - get model inspirations from papers on nlp.stanford.edu/projects/snli/\n",
    "  - sequence-to-sequence encoder-decode architecture for producing the right ordering\n",
    "- better training procedure:\n",
    "  - different training algorithms\n",
    "  - dropout on the input and output embeddings (see tf.nn.dropout)\n",
    "  - L2 regularization (see tf.nn.l2_loss)\n",
    "  - gradient clipping (see tf.clip_by_value or tf.clip_by_norm)\n",
    "- model selection:\n",
    "  - early stopping\n",
    "- hyper-parameter optimization (e.g. random search or grid search (expensive!))\n",
    "    - initial learning rate\n",
    "    - dropout probability\n",
    "    - input and output size\n",
    "    - L2 regularization\n",
    "    - gradient clipping value\n",
    "    - batch size\n",
    "    - ...\n",
    "- post-processing\n",
    "  - for incorporating consistency constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "It is important that this file is placed in the **correct directory**. It will not run otherwise. The correct directory is\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2016/assignment3/problem/group_X/\n",
    "    \n",
    "where `DIRECTORY_OF_YOUR_BOOK` is a placeholder for the directory you downloaded the book to, and in `X` in `group_X` contains the number of your group.\n",
    "\n",
    "After you placed it there, **rename the notebook file** to `group_X`.\n",
    "\n",
    "The notebook is pre-set to save models in\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2016/assignment3/problem/group_X/model/\n",
    "\n",
    "Be sure not to tinker with that - we expect your submission to contain a `model` subdirectory with a single saved model! \n",
    "The saving procedure might overwrite the latest save, or not. Make sure you understand what it does, and upload only a single model! (for more details check tf.train.Saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Instructions\n",
    "This notebook will be used by you to provide your solution, and by us to both assess your solution and enter your marks. It contains three types of sections:\n",
    "\n",
    "1. **Setup** Sections: these sections set up code and resources for assessment. **Do not edit, move nor copy these cells**.\n",
    "2. **Assessment** Sections: these sections are used for both evaluating the output of your code, and for markers to enter their marks. **Do not edit, move, nor copy these cells**.\n",
    "3. **Task** Sections: these sections require your solutions. They may contain stub code, and you are expected to edit this code. For free text answers simply edit the markdown field.  \n",
    "\n",
    "**If you edit, move or copy any of the setup, assessments and mark cells, you will be penalised with -20 points**.\n",
    "\n",
    "Note that you are free to **create additional notebook cells** within a task section. \n",
    "\n",
    "Please **do not share** this assignment nor the dataset publicly, by uploading it online, emailing it to friends etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "To submit your solution:\n",
    "\n",
    "* Make sure that your solution is fully contained in this notebook. Make sure you do not use any additional files other than your saved model.\n",
    "* Make sure that your solution runs linearly from start to end (no execution hops). We will run your notebook in that order.\n",
    "* **Before you submit, make sure your submission is tested on the stat-nlp-book Docker setup to ensure that it does not have any dependencies outside of those that we provide. If your submission fails to adhere to this requirement, you will get 0 points**.\n",
    "* **If running your notebook produces a trivially fixable error that we spot, we will correct it and penalise you with -20 points. Otherwise you will get 0 points for that solution.**\n",
    "* **Rename this notebook to your `group_X`** (where `X` is the number of your group), and adhere to the directory structure requirements, if you have not already done so. ** Failure to do so will result in -1 point.**\n",
    "* Download the notebook in Jupyter via *File -> Download as -> Notebook (.ipynb)*.\n",
    "* Your submission should be a zip file containing the `group_X` directory, containing `group_X.ipynb` notebook, and the `model` directory with _____\n",
    "* Upload that file to the Moodle submission site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries\n",
    "This cell loads libraries important for evaluation and assessment of your model. **Do not change, move or copy it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:56.249298",
     "start_time": "2016-12-20T12:04:54.376398"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib inline\n",
    "#! SETUP 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../../\"\n",
    "sys.path.append(_snlp_book_dir)\n",
    "# docker image contains tensorflow 0.10.0rc0. We will support execution of only that version!\n",
    "import statnlpbook.nn as nn\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Load Training Data\n",
    "\n",
    "This cell loads the training data. **Do not edit the next cell, nor copy/duplicate it**. Instead refer to the variables in your own code, and slice and dice them as you see fit (but do not change their values). \n",
    "For example, no one stops you from introducing, in the corresponding task section, `my_train` and `my_dev` variables that split the data into different folds.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:57.110195",
     "start_time": "2016-12-20T12:04:56.251082"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#! SETUP 2 - DO NOT CHANGE, MOVE NOR COPY\n",
    "data_path = _snlp_book_dir + \"data/nn/\"\n",
    "data_train = nn.load_corpus(data_path + \"train.tsv\")\n",
    "data_dev = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "assert(len(data_train) == 45502)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures\n",
    "\n",
    "Notice that the data is loaded from tab-separated files. The files are easy to read, and we provide the loading functions that load it into a simple data structure. Feel free to check details of the loading.\n",
    "\n",
    "The data structure at hand is an array of dictionaries, each containing a `story` and the `order` entry. `story` is a list of strings, and `order` is a list of integer indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:57.134033",
     "start_time": "2016-12-20T12:04:57.115270"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order': [3, 2, 1, 0, 4],\n",
       " 'story': ['His parents understood and decided to make a change.',\n",
       "  'The doctors told his parents it was unhealthy.',\n",
       "  'Dan was overweight as well.',\n",
       "  \"Dan's parents were overweight.\",\n",
       "  'They got themselves and Dan on a diet.']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1</font>: Model implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Modified preprocessing pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "\n",
    "def tokenize(input):\n",
    "    return re.split(\"-| \",input.replace(\"'s\",\" 's\").replace('.',' .').replace(',',' ,').replace('?',' ?').replace('!',' !'))\n",
    "\n",
    "def pipeline(data, vocab=None, max_sent_len_=None):\n",
    "    is_ext_vocab = True\n",
    "    if vocab is None:\n",
    "        is_ext_vocab = False\n",
    "        vocab = {'<PAD>': 0, '<OOV>': 1}\n",
    "\n",
    "    max_sent_len = -1\n",
    "    data_sentences = []\n",
    "    data_sentences_correctseq = []\n",
    "    data_crqseq_lengths = []\n",
    "    data_orders = []\n",
    "    data_lengths = []\n",
    "    for instance in data:\n",
    "        correctseq=collections.defaultdict(list)\n",
    "        sents = []\n",
    "        lengths = []\n",
    "        for index,sentence in enumerate(instance['story']):\n",
    "            sent = []\n",
    "            length = []\n",
    "            tokenized = tokenize(sentence)\n",
    "            for token in tokenized:\n",
    "                token = token.lower()\n",
    "                if not is_ext_vocab and token not in vocab:\n",
    "          \n",
    "                    vocab[token] = len(vocab)\n",
    "                if token not in vocab:\n",
    "                    token_id = vocab['<OOV>']\n",
    "                else:\n",
    "                    token_id = vocab[token]\n",
    "                sent.append(token_id)\n",
    "            if len(sent) > max_sent_len:\n",
    "                max_sent_len = len(sent)\n",
    "            sents.append(sent)\n",
    "            correctseq[instance['order'][index]]=sent          \n",
    "            lengths.append(len(sent))\n",
    "        #newlist = sorted(list_to_be_sorted, key=lambda k: k['name']) \n",
    "        data_lengths.append(lengths)\n",
    "        data_sentences.append(sents)\n",
    "        data_sentences_correctseq.append([value for key,value in correctseq.items()])\n",
    "        data_crqseq_lengths.append([len(value) for key,value in correctseq.items()])\n",
    "        data_orders.append(instance['order'])\n",
    "\n",
    "    if max_sent_len_ is not None:\n",
    "        max_sent_len = max_sent_len_\n",
    "    out_sentences = np.full([len(data_sentences), 5, max_sent_len], vocab['<PAD>'], dtype=np.int32)\n",
    "    out_sentences_correctseq = np.full([len(data_sentences), 5, max_sent_len], vocab['<PAD>'], dtype=np.int32)\n",
    "    \n",
    "    for i, elem in enumerate(data_sentences):\n",
    "        for j, sent in enumerate(elem):\n",
    "            out_sentences[i, j, 0:len(sent)] = sent\n",
    "   \n",
    "    for i, elem in enumerate(data_sentences_correctseq):\n",
    "        for j, sent in enumerate(elem):\n",
    "            out_sentences_correctseq[i, j, 0:len(sent)] = sent\n",
    "    \n",
    "    sentence_place_order=[]\n",
    "    for lst in data_orders:\n",
    "        temp_lst=[]\n",
    "        for i in range(5):\n",
    "            temp_lst.append(lst.index(i))\n",
    "        sentence_place_order.append(temp_lst)\n",
    "        \n",
    "    out_orders = np.array(data_orders, dtype=np.int32)\n",
    "    out_lengths = np.array(data_lengths, dtype=np.int32)\n",
    "    out_sentence_place_order=np.array(sentence_place_order, dtype=np.int32)\n",
    "    out_crqseq_lengths=np.array(data_crqseq_lengths, dtype=np.int32)\n",
    "    return out_sentences,out_sentences_correctseq,out_lengths,out_crqseq_lengths, out_orders,out_sentence_place_order, vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.842961",
     "start_time": "2016-12-20T12:04:57.136946"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# convert train set to integer IDs\n",
    "train_stories,train_stories_seq,train_lengths,train_crtseq_lengths, train_orders,train_positions, vocab = \\\n",
    "pipeline(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.925263",
     "start_time": "2016-12-20T12:04:59.844598"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# get the length of the longest sentence\n",
    "max_sent_len = train_stories.shape[2]\n",
    "\n",
    "# convert dev set to integer IDs, based on the train vocabulary and max_sent_len\n",
    "dev_stories, dev_stories_seq, dev_lengths, dev_crtseq_lengths, dev_orders,dev_positions, _ = \\\n",
    "pipeline(data_dev, vocab=vocab, max_sent_len_=max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Set up the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.966529",
     "start_time": "2016-12-20T12:04:59.956638"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "target_size = 5\n",
    "vocab_size = len(vocab)\n",
    "input_size = 50*6\n",
    "n, sentence_set_length,max_length = train_stories.shape\n",
    "projection_size = 5 \n",
    "output_size = 5\n",
    "attention_size = 300\n",
    "encoder_hidden_size = 500\n",
    "decoder_hidden_size = 500\n",
    "\n",
    "dropout_rate=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the GloVe word representation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import collections\n",
    "import operator\n",
    "import random\n",
    "\n",
    "word_dict= collections.defaultdict(list)\n",
    "file= open(data_path+'glove.840B.300d.txt', 'r', encoding='utf-8')\n",
    "#file= open(data_path+'glove.twitter.27B.25d.txt', 'r', encoding='utf-8')\n",
    "for line in file:\n",
    "    line = line.rstrip().split(' ')  \n",
    "    word_dict[line[0]]=[float(i) for i in line[1:]]\n",
    "    \n",
    "word_dict=dict(word_dict)\n",
    "\n",
    "sorted_v = sorted(vocab.items(), key=operator.itemgetter(1))\n",
    "\n",
    "embedding_list=[]\n",
    "for item in sorted_v:\n",
    "    if item[0]== '<PAD>':\n",
    "        embedding_list.append(np.array([0 for i in range(300)], dtype='f'))\n",
    "    elif item[0] in word_dict:\n",
    "        embedding_list.append(word_dict[item[0]])\n",
    "    else:\n",
    "        embedding_list.append([random.uniform(-0.5, 0.5) for i in range(300)])\n",
    "            \n",
    "W = np.array(embedding_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#W = np.load('Glove_840B_300.npy')[:vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(orders_gold, orders_predicted,i):\n",
    "    shape = np.shape(orders_predicted)[1]\n",
    "    num =orders_predicted == orders_gold\n",
    "    num_correct = np.sum(np.split(num,shape,axis=1)[i])\n",
    "    num_total =  orders_gold.shape[0]\n",
    "    return num_correct / num_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'split_dim' of 'Split' Op has type int64 that does not match expected type of int32.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    775\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype int64: 'Tensor(\"crt_length:0\", shape=(?, ?), dtype=int64)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e98dc3271c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m                       for sentence in sentences_crtseq] \n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mreshape_crt_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrt_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# 5 times [batch_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0msentences_crtseq_embedded_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_crtseq_embedded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(value, num_or_size_splits, axis, num, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msize_splits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize_splits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m     return gen_array_ops._split(\n\u001b[0;32m-> 1265\u001b[0;31m         split_dim=axis, num_split=num_or_size_splits, value=value, name=name)\n\u001b[0m\u001b[1;32m   1266\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_split\u001b[0;34m(split_dim, value, num_split, name)\u001b[0m\n\u001b[1;32m   5092\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5093\u001b[0m         \u001b[0;34m\"Split\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5094\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   5095\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5096\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDT_INVALID\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m               raise TypeError(\"%s expected type of %s.\" %\n\u001b[0;32m--> 533\u001b[0;31m                               (prefix, dtypes.as_dtype(input_arg.type).name))\n\u001b[0m\u001b[1;32m    534\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m               \u001b[0;31m# Update the maps with the default, if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'split_dim' of 'Split' Op has type int64 that does not match expected type of int32."
     ]
    }
   ],
   "source": [
    "story = tf.placeholder(tf.int64, [None, None, None], \"story\")        # [batch_size x 5 x max_length]\n",
    "length = tf.placeholder(tf.int64, [None, None], \"length\")             # [batch_size x 5] \n",
    "order = tf.placeholder(tf.int64, [None, None], \"order\")              # [batch_size x 5]\n",
    "  \n",
    "batch_size = tf.shape(story)[0]\n",
    "\n",
    "sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(1, 5, story)]  # 5 times [batch_size x max_length]\n",
    "reshape_length = [tf.reshape(x,[-1]) for x in tf.split(1,5, length)]   # 5 times [batch_size]\n",
    "\n",
    "## https://www.tensorflow.org/api_docs/python/array_ops/shapes_and_shaping#reshape\n",
    "# Word embeddings\n",
    "initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "embeddings = tf.get_variable(\"W\", [vocab_size, input_size], initializer=initializer,trainable= False)\n",
    "#embeddings = embeddings.assign(W) \n",
    "\n",
    "sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence)  # [batch_size x max_seq_length x input_size]  \n",
    "                      for sentence in sentences]              # 5 times[batch_size x max_seq_length x input_size]\n",
    "\n",
    "sentences_embedded_1 = sentences_embedded[0] #[batch_size x max_seq_length x input_size]\n",
    "sentences_embedded_2 = sentences_embedded[1]\n",
    "sentences_embedded_3 = sentences_embedded[2]\n",
    "sentences_embedded_4 = sentences_embedded[3]\n",
    "sentences_embedded_5 = sentences_embedded[4]\n",
    "\n",
    "sequence_length_1 = reshape_length[0]\n",
    "sequence_length_2 = reshape_length[1]\n",
    "sequence_length_3 = reshape_length[2]\n",
    "sequence_length_4 = reshape_length[3]\n",
    "sequence_length_5 = reshape_length[4]\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(encoder_hidden_size, state_is_tuple= True)\n",
    "with tf.variable_scope(\"sentence_encoder\") as varscope:        \n",
    "    #stacked_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * 2, state_is_tuple=False)\n",
    "    _, sentences_5_final_state = tf.nn.dynamic_rnn(lstm_cell, sentences_embedded_5,\\\n",
    "                                                   sequence_length=sequence_length_5, dtype=tf.float32)        \n",
    "    sentences_5 = sentences_5_final_state.h  \n",
    "    \n",
    "    varscope.reuse_variables()  \n",
    "    _, sentences_4_final_state = tf.nn.dynamic_rnn(lstm_cell, sentences_embedded_4,  \\\n",
    "                                                   sequence_length=sequence_length_4, dtype=tf.float32)        \n",
    "    sentences_4 = sentences_4_final_state.h\n",
    "\n",
    "    varscope.reuse_variables() \n",
    "    _, sentences_3_final_state = tf.nn.dynamic_rnn(lstm_cell, sentences_embedded_3, \\\n",
    "                                                   sequence_length=sequence_length_3, dtype=tf.float32)        \n",
    "    sentences_3 = sentences_3_final_state.h\n",
    "\n",
    "    varscope.reuse_variables()\n",
    "    _, sentences_2_final_state = tf.nn.dynamic_rnn(lstm_cell, sentences_embedded_2,  \\\n",
    "                                                   sequence_length=sequence_length_2, dtype=tf.float32)        \n",
    "    sentences_2 = sentences_2_final_state.h\n",
    "\n",
    "    varscope.reuse_variables() \n",
    "    _, sentences_1_final_state = tf.nn.dynamic_rnn(lstm_cell, sentences_embedded_1, \\\n",
    "                                                   sequence_length=sequence_length_1, dtype=tf.float32)        \n",
    "    sentences_1 = sentences_1_final_state.h\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#        \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------# \n",
    "\n",
    "story_crtseq = tf.placeholder(tf.int64, [None, None, None], \"story_crtseq\")        # [batch_size x 5 x max_length]\n",
    "crt_length = tf.placeholder(tf.int64, [None, None], \"crt_length\")             # [batch_size x 5] \n",
    "\n",
    "sentences_crtseq = [tf.reshape(x, [batch_size, -1]) for x in tf.split(1, 5, story)] \n",
    "sentences_crtseq_embedded = [tf.nn.embedding_lookup(embeddings, sentence)  # [batch_size x max_seq_length x input_size]  \n",
    "                      for sentence in sentences_crtseq] \n",
    "    \n",
    "reshape_crt_length = [tf.reshape(x,[-1]) for x in tf.split(1,5, crt_length)]   # 5 times [batch_size]\n",
    "\n",
    "sentences_crtseq_embedded_1 = sentences_crtseq_embedded[0] \n",
    "sentences_crtseq_embedded_2 = sentences_crtseq_embedded[1]\n",
    "sentences_crtseq_embedded_3 = sentences_crtseq_embedded[2]\n",
    "sentences_crtseq_embedded_4 = sentences_crtseq_embedded[3]\n",
    "sentences_crtseq_embedded_5 = sentences_crtseq_embedded[4]        \n",
    "\n",
    "crt_sequence_length_1 = reshape_crt_length[0]\n",
    "crt_sequence_length_2 = reshape_crt_length[1]\n",
    "crt_sequence_length_3 = reshape_crt_length[2]\n",
    "crt_sequence_length_4 = reshape_crt_length[3]\n",
    "crt_sequence_length_5 = reshape_crt_length[4] \n",
    "\n",
    "with tf.variable_scope(\"sentence_encoder\") as varscope:  \n",
    "    varscope.reuse_variables() \n",
    "    _, sentences_crtseq_5_final_state = tf.nn.dynamic_rnn(lstm_cell, sentences_crtseq_embedded_5, \\\n",
    "                                                   sequence_length=crt_sequence_length_5, dtype=tf.float32)        \n",
    "    sentences_crtseq_5 = sentences_crtseq_5_final_state.h\n",
    "    \n",
    "    varscope.reuse_variables()  \n",
    "    _, sentences_crtseq_4_final_state = tf.nn.dynamic_rnn(lstm_cell, sentences_crtseq_embedded_4,  \\\n",
    "                                                   sequence_length=crt_sequence_length_4, dtype=tf.float32)        \n",
    "    sentences_crtseq_4 = sentences_crtseq_4_final_state.h\n",
    "\n",
    "    varscope.reuse_variables() \n",
    "    _, sentences_crtseq_3_final_state = tf.nn.dynamic_rnn(lstm_cell, sentences_crtseq_embedded_3, \\\n",
    "                                                   sequence_length=crt_sequence_length_3, dtype=tf.float32)        \n",
    "    sentences_crtseq_3 = sentences_crtseq_3_final_state.h\n",
    "\n",
    "    varscope.reuse_variables()\n",
    "    _, sentences_crtseq_2_final_state = tf.nn.dynamic_rnn(lstm_cell, sentences_crtseq_embedded_2,  \\\n",
    "                                                   sequence_length=crt_sequence_length_2, dtype=tf.float32)        \n",
    "    sentences_crtseq_2 = sentences_crtseq_2_final_state.h\n",
    "\n",
    "    varscope.reuse_variables() \n",
    "    _, sentences_crtseq_1_final_state = tf.nn.dynamic_rnn(lstm_cell, sentences_crtseq_embedded_1,\\\n",
    "                                                   sequence_length=crt_sequence_length_1, dtype=tf.float32)        \n",
    "    sentences_crtseq_1 = sentences_crtseq_1_final_state.h\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#        \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------# \n",
    "sentence_pack = tf.pack([sentences_1,sentences_2,sentences_3,sentences_4,sentences_5],axis=1)    \n",
    "ones = tf.pack([tf.ones([batch_size,encoder_hidden_size], dtype=tf.float32)], axis=1) \n",
    "zeros = tf.pack([tf.zeros([batch_size,encoder_hidden_size], dtype=tf.float32)], axis=1) \n",
    "\n",
    "seq_decoder_1 = tf.pack([sentences_crtseq_1], axis=1) \n",
    "seq_decoder_2 = tf.pack([sentences_crtseq_2], axis=1) \n",
    "seq_decoder_3 = tf.pack([sentences_crtseq_3], axis=1) \n",
    "seq_decoder_4 = tf.pack([sentences_crtseq_4], axis=1) \n",
    "seq_decoder_5 = tf.pack([sentences_crtseq_5], axis=1) \n",
    "\n",
    "def score_functions(s,h, weight, bias):\n",
    "    # h :[batch_size, hidden_size(500)]\n",
    "    inside_part =tf.add(h*weight,bias) \n",
    "    return tf.reduce_sum(inside_part * s,1)\n",
    "\n",
    "score_weight = tf.Variable(tf.random_normal([decoder_hidden_size]),name=\"score_weight\")\n",
    "\n",
    "def score_sumup(s,a):\n",
    "    a_reshape = tf.pack([a],axis=2)\n",
    "    dot_product = s*a_reshape\n",
    "    return tf.reduce_sum(dot_product,1)    \n",
    "\n",
    "score_weight = tf.Variable(tf.random_normal([decoder_hidden_size]),name=\"score_weight\")\n",
    "score_bias   = tf.Variable(tf.random_normal([1]),name=\"score_bias\")\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"encoder\") as varscope:  \n",
    "    _, encoder_hidden_state_initial = tf.nn.dynamic_rnn(lstm_cell,zeros, dtype=tf.float32)  \n",
    "    encoder_hidden_state_0_h = encoder_hidden_state_initial.h\n",
    "    \n",
    "en_0_1 = score_functions(sentences_1,encoder_hidden_state_0_h,score_weight, score_bias)\n",
    "en_0_2 = score_functions(sentences_2,encoder_hidden_state_0_h,score_weight, score_bias)\n",
    "en_0_3 = score_functions(sentences_3,encoder_hidden_state_0_h,score_weight, score_bias)\n",
    "en_0_4 = score_functions(sentences_4,encoder_hidden_state_0_h,score_weight, score_bias)\n",
    "en_0_5 = score_functions(sentences_5,encoder_hidden_state_0_h,score_weight, score_bias)\n",
    "\n",
    "en_0 = tf.nn.softmax(tf.concat(1,tf.pack([en_0_1,en_0_2,en_0_3,en_0_4,en_0_5], axis=1)))\n",
    "s = tf.pack([score_sumup(sentence_pack,en_0)],1)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with tf.variable_scope(\"encoder\") as varscope:  \n",
    "        varscope.reuse_variables()  \n",
    "        _, encoder_hidden_state = tf.nn.dynamic_rnn(lstm_cell,s,initial_state = encoder_hidden_state_initial, dtype=tf.float32)  \n",
    "        encoder_hidden_state_h_hat = encoder_hidden_state.h\n",
    "        encoder_hidden_state_c = encoder_hidden_state.c\n",
    "    \n",
    "    en_1_1 = score_functions(sentences_1,encoder_hidden_state_h_hat,score_weight, score_bias)\n",
    "    en_1_2 = score_functions(sentences_2,encoder_hidden_state_h_hat,score_weight, score_bias)\n",
    "    en_1_3 = score_functions(sentences_3,encoder_hidden_state_h_hat,score_weight, score_bias)\n",
    "    en_1_4 = score_functions(sentences_4,encoder_hidden_state_h_hat,score_weight, score_bias)\n",
    "    en_1_5 = score_functions(sentences_5,encoder_hidden_state_h_hat,score_weight, score_bias)\n",
    "\n",
    "    en_1 = tf.nn.softmax(tf.concat(1,tf.pack([en_1_1,en_1_2,en_1_3,en_1_4,en_1_5], axis=1)))\n",
    "    s = tf.pack([score_sumup(sentence_pack,en_1)],1)\n",
    "    \n",
    "    encoder_hidden_state_initial = encoder_hidden_state\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#        \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#     \n",
    "initial_decoder_parameters = tf.Variable(tf.random_normal([encoder_hidden_size]),name=\"initial_decoder_parameters\")\n",
    "initial_decoder = ones * initial_decoder_parameters\n",
    "\n",
    "lstm_decoder_cell = tf.nn.rnn_cell.LSTMCell(decoder_hidden_size, state_is_tuple= True)\n",
    "lstm_decoder_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_decoder_cell, output_keep_prob=dropout_rate)\n",
    "with tf.variable_scope(\"decoder\") as varscope:         \n",
    "    _, decoder_final_state_5= tf.nn.dynamic_rnn(lstm_decoder_cell, zeros,\\\n",
    "                                                 initial_state =  encoder_hidden_state, dtype=tf.float32)\n",
    "    decoder_hidden_5 = decoder_final_state_5.h\n",
    "    \n",
    "    varscope.reuse_variables()  \n",
    "    _, decoder_final_state_4 = tf.nn.dynamic_rnn(lstm_decoder_cell, seq_decoder_1,\\\n",
    "                                                 initial_state = decoder_final_state_5,dtype=tf.float32)\n",
    "    decoder_hidden_4 = decoder_final_state_4.h\n",
    "    \n",
    "    varscope.reuse_variables() \n",
    "    _, decoder_final_state_3= tf.nn.dynamic_rnn(lstm_decoder_cell, seq_decoder_2,\\\n",
    "                                                initial_state = decoder_final_state_4,dtype=tf.float32)\n",
    "    decoder_hidden_3 = decoder_final_state_3.h\n",
    "    \n",
    "    varscope.reuse_variables() \n",
    "    _, decoder_final_state_2= tf.nn.dynamic_rnn(lstm_decoder_cell, seq_decoder_3,\\\n",
    "                                                initial_state = decoder_final_state_3,dtype=tf.float32)\n",
    "    decoder_hidden_2 = decoder_final_state_2.h\n",
    "    \n",
    "    varscope.reuse_variables() \n",
    "    _, decoder_final_state_1= tf.nn.dynamic_rnn(lstm_decoder_cell, seq_decoder_4,\\\n",
    "                                                initial_state = decoder_final_state_2,dtype=tf.float32)\n",
    "    decoder_hidden_1 = decoder_final_state_1.h\n",
    "    \n",
    "\n",
    "decoder_hidden_5=tf.nn.dropout(decoder_hidden_5,dropout_rate)\n",
    "decoder_hidden_4=tf.nn.dropout(decoder_hidden_4,dropout_rate)\n",
    "decoder_hidden_3=tf.nn.dropout(decoder_hidden_3,dropout_rate)\n",
    "decoder_hidden_2=tf.nn.dropout(decoder_hidden_2,dropout_rate)\n",
    "decoder_hidden_1=tf.nn.dropout(decoder_hidden_1,dropout_rate)\n",
    "\n",
    "e_5_1 = score_functions(sentences_1,decoder_hidden_5, score_weight, score_bias)\n",
    "e_5_2 = score_functions(sentences_2,decoder_hidden_5, score_weight, score_bias)\n",
    "e_5_3 = score_functions(sentences_3,decoder_hidden_5, score_weight, score_bias)\n",
    "e_5_4 = score_functions(sentences_4,decoder_hidden_5, score_weight, score_bias)\n",
    "e_5_5 = score_functions(sentences_5,decoder_hidden_5, score_weight, score_bias)   \n",
    "a_5 = tf.concat(1,tf.pack([e_5_1,e_5_2,e_5_3,e_5_4,e_5_5], axis=1))\n",
    "a_5 = tf.nn.dropout(a_5,0.75)\n",
    "\n",
    "e_4_1 = score_functions(sentences_1,decoder_hidden_4, score_weight, score_bias)\n",
    "e_4_2 = score_functions(sentences_2,decoder_hidden_4, score_weight, score_bias)\n",
    "e_4_3 = score_functions(sentences_3,decoder_hidden_4, score_weight, score_bias)\n",
    "e_4_4 = score_functions(sentences_4,decoder_hidden_4, score_weight, score_bias)\n",
    "e_4_5 = score_functions(sentences_5,decoder_hidden_4, score_weight, score_bias)\n",
    "a_4 = tf.concat(1,tf.pack([e_4_1,e_4_2,e_4_3,e_4_4,e_4_5], axis=1))\n",
    "a_4 = tf.nn.dropout(a_4,0.75)\n",
    "\n",
    "e_3_1 = score_functions(sentences_1,decoder_hidden_3, score_weight, score_bias)\n",
    "e_3_2 = score_functions(sentences_2,decoder_hidden_3, score_weight, score_bias)\n",
    "e_3_3 = score_functions(sentences_3,decoder_hidden_3, score_weight, score_bias)\n",
    "e_3_4 = score_functions(sentences_4,decoder_hidden_3, score_weight, score_bias)\n",
    "e_3_5 = score_functions(sentences_5,decoder_hidden_3, score_weight, score_bias)\n",
    "a_3 = tf.concat(1,tf.pack([e_3_1,e_3_2,e_3_3,e_3_4,e_3_5], axis=1))\n",
    "a_3 = tf.nn.dropout(a_3,0.75)\n",
    "\n",
    "e_2_1 = score_functions(sentences_1,decoder_hidden_2, score_weight, score_bias)\n",
    "e_2_2 = score_functions(sentences_2,decoder_hidden_2, score_weight, score_bias)\n",
    "e_2_3 = score_functions(sentences_3,decoder_hidden_2, score_weight, score_bias)\n",
    "e_2_4 = score_functions(sentences_4,decoder_hidden_2, score_weight, score_bias)\n",
    "e_2_5 = score_functions(sentences_5,decoder_hidden_2, score_weight, score_bias)\n",
    "a_2 = tf.concat(1,tf.pack([e_2_1,e_2_2,e_2_3,e_2_4,e_2_5], axis=1))\n",
    "a_2 = tf.nn.dropout(a_2,0.75)\n",
    "\n",
    "e_1_1 = score_functions(sentences_1,decoder_hidden_1, score_weight, score_bias)\n",
    "e_1_2 = score_functions(sentences_2,decoder_hidden_1, score_weight, score_bias)\n",
    "e_1_3 = score_functions(sentences_3,decoder_hidden_1, score_weight, score_bias)\n",
    "e_1_4 = score_functions(sentences_4,decoder_hidden_1, score_weight, score_bias)\n",
    "e_1_5 = score_functions(sentences_5,decoder_hidden_1, score_weight, score_bias)\n",
    "a_1 = tf.concat(1,tf.pack([e_1_1,e_1_2,e_1_3,e_1_4,e_1_5], axis=1))\n",
    "a_1 = tf.nn.dropout(a_1,0.75)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#        \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#     \n",
    "\n",
    "position = tf.placeholder(tf.int64, [None, None], \"position\")             # [batch_size x 5]    \n",
    "\n",
    "reshape_position = [tf.reshape(x,[-1]) for x in tf.split(1,5, position)] \n",
    "position_1 = reshape_position[0]\n",
    "position_2 = reshape_position[1]\n",
    "position_3 = reshape_position[2]\n",
    "position_4 = reshape_position[3]\n",
    "position_5 = reshape_position[4]\n",
    "\n",
    "loss_1 = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(a_5, position_1))\n",
    "loss_2 = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(a_4, position_2))\n",
    "loss_3 = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(a_3, position_3))\n",
    "loss_4 = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(a_2, position_4))\n",
    "loss_5 = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(a_1, position_5))\n",
    "\n",
    "loss_2 =  0.8*loss_1+ 0.8* loss_2 + 1.5*loss_3 + 1.5*loss_4+ 1.2*loss_5 \n",
    "opt_op_2 = tf.train.AdamOptimizer(1e-3).minimize(loss_2)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#        \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#     \n",
    "sentence_encoder = tf.pack([sentences_1,sentences_2,sentences_3,sentences_4,sentences_5], axis=1)  #[batch_size x 5 x encoder_hidden_size]\n",
    "\n",
    "def setence_select(logits, indices):\n",
    "    batch_size = tf.shape(logits)[0]\n",
    "    rows_per_batch = tf.shape(logits)[1]\n",
    "    indices_per_batch = tf.shape(indices)[1]\n",
    "\n",
    "    # Offset to add to each row in indices. We use `tf.expand_dims()` to make \n",
    "    # this broadcast appropriately.\n",
    "    offset = tf.expand_dims(tf.range(0, batch_size) * rows_per_batch, 1)\n",
    "    \n",
    "    # Convert indices and logits into appropriate form for `tf.gather()`. \n",
    "    flattened_indices = tf.reshape(indices + offset, [-1])\n",
    "    flattened_logits = tf.reshape(logits, tf.concat(0, [[-1], tf.shape(logits)[2:]]))\n",
    "    \n",
    "    selected_rows = tf.gather(flattened_logits, flattened_indices)\n",
    "\n",
    "    return tf.reshape(selected_rows,tf.concat(0, [tf.pack([batch_size, indices_per_batch]), tf.shape(logits)[2:]]))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#        \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#     \n",
    "        \n",
    "with tf.variable_scope(\"decoder\") as varscope:    \n",
    "    varscope.reuse_variables() \n",
    "    _, decoder_predict_5= tf.nn.dynamic_rnn(lstm_decoder_cell, zeros,\\\n",
    "                                            initial_state =  encoder_hidden_state, dtype=tf.float32)\n",
    "    predict_hidden_state_5 = decoder_predict_5.h\n",
    "\n",
    "p_5_1 = score_functions(sentences_1, predict_hidden_state_5, score_weight, score_bias)\n",
    "p_5_2 = score_functions(sentences_2, predict_hidden_state_5, score_weight, score_bias)\n",
    "p_5_3 = score_functions(sentences_3, predict_hidden_state_5, score_weight, score_bias)\n",
    "p_5_4 = score_functions(sentences_4, predict_hidden_state_5, score_weight, score_bias)\n",
    "p_5_5 = score_functions(sentences_5, predict_hidden_state_5, score_weight, score_bias)\n",
    "\n",
    "p_5 = tf.pack([tf.argmax(tf.nn.softmax(tf.concat(1,tf.pack([p_5_1,p_5_2,p_5_3,p_5_4,p_5_5], axis=1))),1)], axis=1)\n",
    "p_5 = tf.cast(p_5, tf.int32)   \n",
    "\n",
    "predicted_sentence_5 = setence_select(sentence_encoder, p_5)\n",
    "predicted_sentence_5.set_shape([None, None, encoder_hidden_size])\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"decoder\") as varscope:    \n",
    "    varscope.reuse_variables() \n",
    "    _, decoder_predict_4= tf.nn.dynamic_rnn(lstm_decoder_cell, predicted_sentence_5, \\\n",
    "                                            initial_state = decoder_predict_5, dtype=tf.float32)\n",
    "    predict_hidden_state_4 = decoder_predict_4.h\n",
    "\n",
    "p_4_1 = score_functions(sentences_1, predict_hidden_state_4, score_weight, score_bias)\n",
    "p_4_2 = score_functions(sentences_2, predict_hidden_state_4, score_weight, score_bias)\n",
    "p_4_3 = score_functions(sentences_3, predict_hidden_state_4, score_weight, score_bias)\n",
    "p_4_4 = score_functions(sentences_4, predict_hidden_state_4, score_weight, score_bias)\n",
    "p_4_5 = score_functions(sentences_5, predict_hidden_state_4, score_weight, score_bias)\n",
    "\n",
    "p_4 = tf.pack([tf.argmax(tf.nn.softmax(tf.concat(1,tf.pack([p_4_1,p_4_2,p_4_3,p_4_4,p_4_5], axis=1))),1)], axis=1)  \n",
    "p_4 = tf.cast(p_4, tf.int32)\n",
    "\n",
    "predicted_sentence_4 = setence_select(sentence_encoder, p_4)\n",
    "predicted_sentence_4.set_shape([None, None, encoder_hidden_size])\n",
    "\n",
    "with tf.variable_scope(\"decoder\") as varscope:    \n",
    "    varscope.reuse_variables() \n",
    "    _, decoder_predict_3= tf.nn.dynamic_rnn(lstm_decoder_cell, predicted_sentence_4, \\\n",
    "                                            initial_state = decoder_predict_4 ,dtype=tf.float32)\n",
    "    predict_hidden_state_3 = decoder_predict_3.h\n",
    "\n",
    "p_3_1 = score_functions(sentences_1,predict_hidden_state_3, score_weight, score_bias)\n",
    "p_3_2 = score_functions(sentences_2,predict_hidden_state_3, score_weight, score_bias)\n",
    "p_3_3 = score_functions(sentences_3,predict_hidden_state_3, score_weight, score_bias)\n",
    "p_3_4 = score_functions(sentences_4,predict_hidden_state_3, score_weight, score_bias)\n",
    "p_3_5 = score_functions(sentences_5,predict_hidden_state_3, score_weight, score_bias)\n",
    "\n",
    "p_3 = tf.pack([tf.argmax(tf.nn.softmax(tf.concat(1,tf.pack([p_3_1,p_3_2,p_3_3,p_3_4,p_3_5], axis=1))),1)], axis=1) \n",
    "p_3 = tf.cast(p_3, tf.int32)\n",
    "\n",
    "predicted_sentence_3 = setence_select(sentence_encoder, p_3)\n",
    "predicted_sentence_3.set_shape([None, None, encoder_hidden_size])\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"decoder\") as varscope:    \n",
    "    varscope.reuse_variables() \n",
    "    _, decoder_predict_2= tf.nn.dynamic_rnn(lstm_decoder_cell, predicted_sentence_3, \\\n",
    "                                            initial_state = decoder_predict_3, dtype=tf.float32)\n",
    "    predict_hidden_state_2 = decoder_predict_2.h\n",
    "\n",
    "p_2_1 = score_functions(sentences_1,predict_hidden_state_2, score_weight, score_bias)\n",
    "p_2_2 = score_functions(sentences_2,predict_hidden_state_2, score_weight, score_bias)\n",
    "p_2_3 = score_functions(sentences_3,predict_hidden_state_2, score_weight, score_bias)\n",
    "p_2_4 = score_functions(sentences_4,predict_hidden_state_2, score_weight, score_bias)\n",
    "p_2_5 = score_functions(sentences_5,predict_hidden_state_2, score_weight, score_bias)\n",
    "\n",
    "p_2 = tf.pack([tf.argmax(tf.nn.softmax(tf.concat(1,tf.pack([p_2_1,p_2_2,p_2_3,p_2_4,p_2_5], axis=1))),1)], axis=1)\n",
    "p_2 = tf.cast(p_2, tf.int32)\n",
    "\n",
    "predicted_sentence_2 = setence_select(sentence_encoder, p_2)\n",
    "predicted_sentence_2.set_shape([None, None, encoder_hidden_size])\n",
    "\n",
    "with tf.variable_scope(\"decoder\") as varscope:    \n",
    "    varscope.reuse_variables() \n",
    "    _, decoder_predict_1= tf.nn.dynamic_rnn(lstm_decoder_cell, predicted_sentence_2, \\\n",
    "                                            initial_state = decoder_predict_2, dtype=tf.float32)\n",
    "    predict_hidden_state_1 = decoder_predict_1.h\n",
    "\n",
    "p_1_1 = score_functions(sentences_1,predict_hidden_state_1, score_weight, score_bias)\n",
    "p_1_2 = score_functions(sentences_2,predict_hidden_state_1, score_weight, score_bias)\n",
    "p_1_3 = score_functions(sentences_3,predict_hidden_state_1, score_weight, score_bias)\n",
    "p_1_4 = score_functions(sentences_4,predict_hidden_state_1, score_weight, score_bias)\n",
    "p_1_5 = score_functions(sentences_5,predict_hidden_state_1, score_weight, score_bias)\n",
    "\n",
    "p_1 = tf.pack([tf.argmax(tf.nn.softmax(tf.concat(1,tf.pack([p_1_1,p_1_2,p_1_3,p_1_4,p_1_5], axis=1))),1)], axis=1)\n",
    "p_1 = tf.cast(p_1, tf.int32)\n",
    "\n",
    "predict = tf.concat(1,[p_5, p_4, p_3, p_2, p_1])   \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:00.995336",
     "start_time": "2016-12-20T12:04:59.968153"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "source": [
    "BATCH_SIZE = 25\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    n = train_stories.shape[0]\n",
    "    dev_feed_dict = {story: dev_stories, length:dev_lengths}\n",
    "        \n",
    "    print('----- Initial -----')\n",
    "    total_loss = 0\n",
    "        \n",
    "    for i in range(n // BATCH_SIZE):\n",
    "        index_list = random.sample(range(len(train_stories)), BATCH_SIZE )\n",
    "        inst_story = [train_stories[idx] for idx in index_list]\n",
    "        inst_length = [train_lengths[idx] for idx in index_list]\n",
    "        inst_order = [train_orders[idx] for idx in index_list]\n",
    "        inst_story_crtseq = [train_stories_seq[idx] for idx in index_list]\n",
    "        inst_position  = [train_positions[idx] for idx in index_list]\n",
    "        inst_crt_length = [train_crtseq_lengths[idx] for idx in index_list]\n",
    "        feed_dict = {story: inst_story,length:inst_length, order: inst_order,\\\n",
    "                    story_crtseq: inst_story_crtseq, position: inst_position, crt_length: inst_crt_length}\n",
    "        _, current_loss_fw = sess.run([opt_op_fw, loss_fw], feed_dict=feed_dict)\n",
    "        _, current_loss_bw = sess.run([opt_op_bw, loss_bw], feed_dict=feed_dict)\n",
    "        total_loss += current_loss_bw + current_loss_fw \n",
    "    print(' Train loss:', total_loss / (2*n))\n",
    "        \n",
    "    dev_predicted = sess.run(predict, feed_dict=dev_feed_dict)\n",
    "    dev_accuracy = nn.calculate_accuracy(dev_positions, dev_predicted)\n",
    "    print(' Dev accuracy:', dev_accuracy)\n",
    "        \n",
    "    print(' Dev accuracy_detailed:',calculate_accuracy(dev_positions, dev_predicted,0),calculate_accuracy(dev_positions, dev_predicted,1),\\\n",
    "        calculate_accuracy(dev_positions, dev_predicted,2),calculate_accuracy(dev_positions, dev_predicted,3),\\\n",
    "        calculate_accuracy(dev_positions, dev_predicted,4))\n",
    "    nn.save_model(sess)\n",
    "    \n",
    "    while calculate_accuracy(dev_positions, dev_predicted,3) + calculate_accuracy(dev_positions, dev_predicted,4) < 0.96:\n",
    "        bw_index=0\n",
    "        print('----- Backward',bw_index , '-----')   \n",
    "        for i in range(n // BATCH_SIZE):\n",
    "            index_list = random.sample(range(len(train_stories)), BATCH_SIZE )\n",
    "            inst_story = [train_stories[idx] for idx in index_list]\n",
    "            inst_length = [train_lengths[idx] for idx in index_list]\n",
    "            inst_order = [train_orders[idx] for idx in index_list]\n",
    "            inst_story_crtseq = [train_stories_seq[idx] for idx in index_list]\n",
    "            inst_position  = [train_positions[idx] for idx in index_list]\n",
    "            inst_crt_length = [train_crtseq_lengths[idx] for idx in index_list]\n",
    "            feed_dict = {story: inst_story,length:inst_length, order: inst_order,\\\n",
    "                        story_crtseq: inst_story_crtseq, position: inst_position, crt_length: inst_crt_length}\n",
    "            _, current_loss_bw = sess.run([opt_op_bw, loss_bw], feed_dict=feed_dict)\n",
    "            total_loss += current_loss_bw + current_loss_fw \n",
    "        print(' Train loss:', total_loss / n)\n",
    "            \n",
    "        dev_predicted = sess.run(predict, feed_dict=dev_feed_dict)\n",
    "        dev_accuracy = nn.calculate_accuracy(dev_positions, dev_predicted)\n",
    "        print(' Dev accuracy:', dev_accuracy)\n",
    "            \n",
    "        print(' Dev accuracy_detailed:',calculate_accuracy(dev_positions, dev_predicted,0),calculate_accuracy(dev_positions, dev_predicted,1),\\\n",
    "            calculate_accuracy(dev_positions, dev_predicted,2),calculate_accuracy(dev_positions, dev_predicted,3),\\\n",
    "            calculate_accuracy(dev_positions, dev_predicted,4))\n",
    "        bw_index += 1\n",
    "        nn.save_model(sess)\n",
    "        \n",
    "    for epoch in range(15):\n",
    "        print('----- Epoch', epoch, '-----')\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(n // BATCH_SIZE):\n",
    "            index_list = random.sample(range(len(train_stories)), BATCH_SIZE )\n",
    "            inst_story = [train_stories[idx] for idx in index_list]\n",
    "            inst_length = [train_lengths[idx] for idx in index_list]\n",
    "            inst_order = [train_orders[idx] for idx in index_list]\n",
    "            inst_story_crtseq = [train_stories_seq[idx] for idx in index_list]\n",
    "            inst_position  = [train_positions[idx] for idx in index_list]\n",
    "            inst_crt_length = [train_crtseq_lengths[idx] for idx in index_list]\n",
    "            feed_dict = {story: inst_story,length:inst_length, order: inst_order,\\\n",
    "                        story_crtseq: inst_story_crtseq, position: inst_position, crt_length: inst_crt_length}\n",
    "            _, current_loss_fw = sess.run([opt_op_fw, loss_fw], feed_dict=feed_dict)\n",
    "            total_loss += current_loss_bw + current_loss_fw \n",
    "        print(' Train loss:', total_loss / n)  \n",
    "        \n",
    "        dev_predicted = sess.run(predict, feed_dict=dev_feed_dict)\n",
    "        dev_accuracy = nn.calculate_accuracy(dev_positions, dev_predicted)\n",
    "        print(' Dev accuracy:', dev_accuracy)\n",
    "            \n",
    "        print(' Dev accuracy_detailed:',calculate_accuracy(dev_positions, dev_predicted,0),calculate_accuracy(dev_positions, dev_predicted,1),\\\n",
    "            calculate_accuracy(dev_positions, dev_predicted,2),calculate_accuracy(dev_positions, dev_predicted,3),\\\n",
    "            calculate_accuracy(dev_positions, dev_predicted,4))\n",
    "        \n",
    "        nn.save_model(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1</font>: Assess Accuracy (50 pts) \n",
    "\n",
    "We assess how well your model performs on an unseen test set. We will look at the accuracy of the predicted sentence order, on sentence level, and will score them as followis:\n",
    "\n",
    "* 0 - 20 pts: 45% <= accuracy < 50%, linear\n",
    "* 20 - 40 pts: 50% <= accuracy < 55\n",
    "* 40 - 70 pts 55 <= accuracy < Best Result, linear\n",
    "\n",
    "The **linear** mapping maps any accuracy value between the lower and upper bound linearly to a score. For example, if your model's accuracy score is $acc=54.5\\%$, then your score is $20 + 20\\frac{acc-50}{55-50}$.\n",
    "\n",
    "The *Best-Result* accuracy is the maximum of the best accuracy the course organiser achieved, and the submitted accuracies scores.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Change the following lines so that they construct the test set in the same way you constructed the dev set in the code above. We will insert the test set instead of the dev set here. test_feed_dict variable must stay named the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:54.755730",
     "start_time": "2016-12-20T12:05:54.617471"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# LOAD THE DATA\n",
    "data_test = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "# make sure you process this with the same pipeline as you processed your dev set\n",
    "test_stories, test_stories_seq, test_lengths, test_crtseq_lengths,_,test_orders, _ = \\\n",
    "pipeline(data_test, vocab=vocab, max_sent_len_=max_sent_len)\n",
    "\n",
    "# THIS VARIABLE MUST BE NAMED `test_feed_dict`\n",
    "test_feed_dict = {story: test_stories, length: test_lengths}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads your model, computes accuracy, and exports the result. **DO NOT** change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:55.116609",
     "start_time": "2016-12-20T12:05:54.758571"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#! ASSESSMENT 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "with tf.Session() as sess:\n",
    "    # LOAD THE MODEL\n",
    "    saver = tf.train.Saver()   \n",
    "    saver.restore(sess, './model/model.checkpoint')\n",
    "    \n",
    "    # RUN TEST SET EVALUATION\n",
    "    test_predicted = sess.run(predict, feed_dict=test_feed_dict)\n",
    "    test_accuracy = nn.calculate_accuracy(test_orders, test_predicted)\n",
    "\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Mark</font>:  Your solution to Task 1 is marked with ** __ points**. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2</font>: Describe your Approach\n",
    "\n",
    "Enter a 750 words max description of your approach **in this cell**.\n",
    "Make sure to provide:\n",
    "- an **error analysis** of the types of errors your system makes\n",
    "- compare your system with the model we provide, focus on differences and draw useful comparations between them\n",
    "\n",
    "Should you need to include figures in your report, make sure they are Python-generated. For that, feel free to create new cells after this cell (before Assessment 2 cell). Link online images at your risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Model Description:\n",
    "The basic idea of our model is to use pre-trained word embedding to capture semantics and input into a RNN model for sequence modelling. The approach is inspired by the method introduced in [Sentence Ordering using Recurrent Neural Networks ( Logeswaran et al, 2017)](https://arxiv.org/pdf/1611.02654v1.pdf). The model is comprised of a sentence encoder RNN, an encoder RNN and a decoder RNN:\n",
    "\n",
    "**Sentence Encoder**: Pipeline function is modified so that it can produce the correct order and length of sentences for the sequence to sequence model. After processing the pipeline, we construct a RNN as sentence encoder which takes the words of a sentence $“s”$ sequentially as input and computes the sentence representations.\n",
    "\n",
    "**Encoder**: We apply a RNN as encoder which attends to the word embeddings and computes an attention readout at each step, then appending it to the current hidden state. \n",
    "\n",
    "The structure is defined as equation $(1)-(4)$. Initially the LSTM takes a zero vector as input, after updating the regular LSTM hidden state $({h}_{enc}^t,{c}_{enc}^t)$, we compute an attention readout vector $s_{att}^t$ by composing the hidden state with sentence embedding through a scoring function $f$ (Equation $(5)$) and taking the $softmax$ to produce attention probabilities (Equations $(2) - (4)$). The attention readout vector $s_{att}^t$ is then used as LSTM input for the next time step. The process is repeated for certain time.\n",
    "\n",
    "\n",
    "$$(1)\\  {h}_{enc}^t,e{c}_{enc}^t = LSTM (h_{enc}^{t-1},c_{enc}^{t-1},s_{att}^{t-1}) $$\n",
    "\n",
    "$$(2)\\  e_{enc}^{t,i}=f(s_i,{h}_{enc}^t);i\\in \\{1,...,n\\}$$\n",
    "\n",
    "$$(3)\\  a_{enc}^{t} = Softmax(e_{enc}^t)$$\n",
    "\n",
    "$$(4)\\  s_{att}^t = \\sum_{i=1}^n a_{enc}^{t,i} s_i $$\n",
    "\n",
    "$$(5)\\  f(s,h) = s^T (Wh)$$\n",
    "\n",
    "**Decoder**: We construct a new RNN as decoder that produces the target sequence conditioned on the representation produced by the encoder. The attention weights are used by decoder for prediction. \n",
    "\n",
    "The structure of decoder is shown as equation $(6)-(8)$. The LSTM takes the embedding of the previous sentence as input. The attention probability $a_{dec}^{t,i}$ is computed by the same method as the encoder. The initial state of the decoder LSTM is initialized with the final hidden state of the encoder. During training time the correct order of sentences is used as input while during prediction we use the previously predicted sentences. $x^o$ is set as a zero vector.\n",
    "\n",
    "$$(6)\\  h_{dec}^t,c_{dec}^t  =LSTM (h_{dec}^{t-1},c_{dec}^{t-1},x^{t-1}) $$\n",
    "\n",
    "$$(7)\\  e_{dec}^{t,i} = f(s_i,h_{dec}^t); i \\in \\{1,...,n\\}$$\n",
    "\n",
    "$$(8)\\  a_{dec}^t = Softmax(e_{dec}^t) $$\n",
    " \n",
    " \n",
    "$$\n",
    "\\  \n",
    "$$\n",
    "The **Model overview** is as followed ([flickr link](https://www.flickr.com/photos/147273529@N04/31935764933/in/dateposted-public/)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a data-flickr-embed=\"true\"  href=\"https://www.flickr.com/photos/147273529@N04/31935764933/in/dateposted-public/\" title=\"Seq2Seq\"><img src=\"https://c1.staticflickr.com/1/630/31935764933_346c8793e5_k.jpg\" width=\"2048\" height=\"1152\" alt=\"Seq2Seq\"></a><script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## Training and Prediction:\n",
    "**Model Training:**  Pre-trained 300 dimension Glove is used as word embedding, all LSTM cells in encoder and decoder have a hidden layer size of 500. The number of learning iteration in encoder cell is set as 5. We minimise the cross entropy loss by using the Adam optimiser with a learning rate of 1e-4 and batch size is set to be 25. Regularisation is implemented by dropouts the sentence representations and the decoder cells with the rate of 0.5. Early stopping is also applied for regularisation.\n",
    "\n",
    "**Model Predictions:** During test, we found out that if we pass the sentences reversely (ending sentence first and starting sentence last) into decoder during training, it performed better in predicting sentences in the last two orders, while if passing the sentences forwardly then it performed better in predicting sentences in the first two orders. Therefore we concatenated one forward and one backward models, which do not share any parameters, as our final model, so that the first three sentences are predicted by the forward model and the last two sentences are predicted by the backward model, which successfully improved the accuracy by 2%.\n",
    "\n",
    "## Comparison:\n",
    "$\\bullet$ Our model used a completely different predicting structure compares to the provided model, instead of directly predicting which order belongs to each sentence, our model predicts which sentence belongs to each order; in another word, it points orders to sentences instead of pointing sentences to orders. \n",
    "\n",
    "$\\bullet$ Also instead of a direct discriminate approach in the provided model, our model predicts the order in a more interpretable way: the sentence encoders first create the sequence representations, then the encoders read through whole sentences, and finally the decoders predict the sentences by order one by one. \n",
    "\n",
    "$\\bullet$ The generality of our model is stronger than the origin model due to the benefit from the sequence to sequence model; it can be extended easily to predict variable lengths set of sentences. \n",
    "\n",
    "$\\bullet$ Finally, the order of sentences are predicted independently in the provided model, which failed to extract the inter-correlation within the sentences. In our model, the orders are predicted in condition of the previously predicted orders. \n",
    "\n",
    "## Error Analysis:\n",
    "| Type | 1st sentence  | 2nd sentence | 3rd sentence  |4th sentence  |5th sentence  |\n",
    "|------|---|--|--|--|\n",
    "| **Dev Accuracy** | 0.890967397114 |0.595403527525 |0.432923570283  |0.390700160342  |0.539283805452  |\n",
    "The model performs quite well on predicting the first sentences, relatively fine on predicting the second and the last sentences, but it is not good at predicting the third and the forth sentences. Also, 65.3% misclassified 3rd sentences actually belong to the 4th sentences, while 68.5% misclassified 4th sentences are the 3rd sentences. This is consistent with what we human predict the sentence orders, where it is easy to find the first and the last sentences without any prior knowledge, but predicting the third and forth sentence is not an easy task. \n",
    "\n",
    "\n",
    "## Reference:\n",
    "Logeswaran, L., Lee, H. and Radev, D. (2017). Sentence Ordering using Recurrent Neural Networks\n",
    "\n",
    "Ilya Sutskever, Oriol Vinyals, Quoc V. Le (2014). Sequence to Sequence Learning with Neural Networks\n",
    "\n",
    "Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiský, Phil Blunsom(2015). Reasoning about Entailment with Neural Attention\n",
    "\n",
    "Yang Liu, Chengjie Sun, Lei Lin, Xiaolong Wang (2016). Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention\n",
    "\n",
    "\n",
    "$$\n",
    "\\\n",
    "\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 2</font>: Assess Description (30 pts) \n",
    "\n",
    "We will mark the description along the following dimensions: \n",
    "\n",
    "* Clarity (10pts: very clear, 0pts: we can't figure out what you did, or you did nothing)\n",
    "* Creativity (10pts: we could not have come up with this, 0pts: Use only the provided model)\n",
    "* Substance (10pts: implemented complex state-of-the-art classifier, compared it to a simpler model, 0pts: Only use what is already there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Mark</font>:  Your solution to Task 2 is marked with ** __ points**.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Final mark</font>: Your solution to Assignment 3 is marked with ** __points**. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
