{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#! SETUP 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../../\"\n",
    "sys.path.append(_snlp_book_dir)\n",
    "# docker image contains tensorflow 0.10.0rc0. We will support execution of only that version!\n",
    "import statnlpbook.nn as nn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! SETUP 2 - DO NOT CHANGE, MOVE NOR COPY\n",
    "data_path = _snlp_book_dir + \"data/nn/\"\n",
    "data_train = nn.load_corpus(data_path + \"train.tsv\")\n",
    "data_dev = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "assert(len(data_train) == 45502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "# data loading\n",
    "def load_corpus(filename):\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            splits = [x.strip() for x in line.split(\"\\t\")]\n",
    "            current_story = splits[0:5]\n",
    "            current_order = list(int(elem) for elem in splits[5:])\n",
    "            instance = {\"story\": current_story, \"order\": current_order}\n",
    "            data.append(instance)\n",
    "    return data\n",
    "\n",
    "\n",
    "# tokenisation\n",
    "def tokenize(input):\n",
    "    input = string = re.sub('[\\s+\\.\\!\\/_,$%^*(+\\\"\\')]+|[+——()?【】“”！，。？、~@#￥%……&*（）]+', \" \",input)\n",
    "    return input.split(' ')\n",
    "\n",
    "\n",
    "# preprocessing pipeline, used to load the data intro a structure required by the model\n",
    "def pipeline(data, vocab=None, max_sent_len_=None):\n",
    "    is_ext_vocab = True\n",
    "    if vocab is None:\n",
    "        is_ext_vocab = False\n",
    "        vocab = {'<PAD>': 0, '<OOV>': 1}\n",
    "\n",
    "    max_sent_len = -1\n",
    "    data_sentences = []\n",
    "    data_orders = []\n",
    "    data_orders_reverse = []\n",
    "    for instance in data:\n",
    "        sents = []\n",
    "        for sentence in instance['story']:\n",
    "            sent = []\n",
    "            tokenized = tokenize(sentence)\n",
    "            for token in tokenized:\n",
    "                #token = token.lower()\n",
    "                if not is_ext_vocab and token not in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "                if token not in vocab:\n",
    "                    token_id = vocab['<OOV>']\n",
    "                else:\n",
    "                    token_id = vocab[token]\n",
    "                sent.append(token_id)\n",
    "            if len(sent) > max_sent_len:\n",
    "                max_sent_len = len(sent)\n",
    "            sents.append(sent)\n",
    "        data_sentences.append(sents)\n",
    "        data_orders.append(instance['order'])\n",
    "        data_orders_reverse.append(instance['order'][::-1])\n",
    "\n",
    "    if max_sent_len_ is not None:\n",
    "        max_sent_len = max_sent_len_\n",
    "    out_sentences = np.full([len(data_sentences), 5, max_sent_len], vocab['<PAD>'], dtype=np.int32)\n",
    "\n",
    "    for i, elem in enumerate(data_sentences):\n",
    "        for j, sent in enumerate(elem):\n",
    "            out_sentences[i, j, 0:len(sent)] = sent\n",
    "\n",
    "    out_orders = np.array(data_orders, dtype=np.int32)\n",
    "    out_orders_reverse = np.array(data_orders_reverse, dtype=np.int32)\n",
    "\n",
    "    return out_sentences, out_orders, out_orders_reverse,vocab\n",
    "\n",
    "\n",
    "# displaying the loaded data\n",
    "def show_data_instance(data_stories, data_orders, vocab, num_story):\n",
    "    inverted_vocab = {value: key for key, value in vocab.items()}\n",
    "    print('Input:\\n Story:')\n",
    "    story_example = {}\n",
    "    for i, elem in enumerate(data_stories[num_story]):\n",
    "        x = list(inverted_vocab[ch] if ch in inverted_vocab else '<OOV>'\n",
    "                 for ch in elem if ch != 0)\n",
    "        story_example[data_orders[num_story][i]] = \" \".join(x)\n",
    "        print(' ',\" \".join(x))\n",
    "    print(' Order:\\n ', data_orders[num_story])\n",
    "    print('\\nDesired story:')\n",
    "    for (k, v) in sorted(story_example.items()):\n",
    "        print(' ',v)\n",
    "\n",
    "\n",
    "# accuracy calculation\n",
    "def calculate_accuracy(orders_gold, orders_predicted):\n",
    "    num_correct = np.sum(orders_predicted == orders_gold)\n",
    "    num_total =  orders_gold.shape[0] * 5\n",
    "    return num_correct / num_total\n",
    "\n",
    "\n",
    "# save the model params to the hard drive\n",
    "def save_model(session):\n",
    "    if not os.path.exists('./model/'):\n",
    "        os.mkdir('./model/')\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, './model/model.checkpoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stories, train_orders, train_orders_reverse, vocab = pipeline(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " Story:\n",
      "  His parents understood and decided to make a change \n",
      "  The doctors told his parents it was unhealthy \n",
      "  Dan was overweight as well \n",
      "  Dan s parents were overweight \n",
      "  They got themselves and Dan on a diet \n",
      " Order:\n",
      "  [3 2 1 0 4]\n",
      "\n",
      "Desired story:\n",
      "  Dan s parents were overweight \n",
      "  Dan was overweight as well \n",
      "  The doctors told his parents it was unhealthy \n",
      "  His parents understood and decided to make a change \n",
      "  They got themselves and Dan on a diet \n"
     ]
    }
   ],
   "source": [
    "show_data_instance(train_stories,train_orders,vocab,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the length of the longest sentence\n",
    "max_sent_len = train_stories.shape[2]\n",
    "# convert dev set to integer IDs, based on the train vocabulary and max_sent_len\n",
    "dev_stories, dev_orders, dev_order_reverse, _ = pipeline(data_dev, vocab=vocab, max_sent_len_=max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL PARAMETERS ###\n",
    "target_size = 5\n",
    "vocab_size = len(vocab)\n",
    "input_size = 10\n",
    "# n = len(train_stories)\n",
    "output_size = 5\n",
    "\n",
    "\n",
    "rnn_size= 200\n",
    "num_of_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "story = tf.placeholder(tf.int64, [None, None, None], \"story\")\n",
    "order = tf.placeholder(tf.int64, [None, None], \"order\")\n",
    "order_reverse = tf.placeholder(tf.int64, [None, None], \"order_reverse\")\n",
    "\n",
    "batch_size = tf.shape(story)[0]\n",
    "\n",
    "sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(axis=1, num_or_size_splits=5, value=story)]  # 5 times [batch_size x max_length]\n",
    "\n",
    "initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "embeddings = tf.get_variable(\"W\", [vocab_size, input_size], initializer=initializer)\n",
    "\n",
    "sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence) for sentence in sentences]\n",
    "hs1 = [tf.reduce_sum(sentence, 1) for sentence in sentences_embedded] # 5 times [batch_size x input_size]\n",
    "hs2 = hs1[::-1]  #5 times [batch_size x input_size]\n",
    "\n",
    "# Model 1\n",
    "# encoder\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "initial_state1 = state1 = lstm_cell.zero_state(batch_size,tf.float32)\n",
    "\n",
    "for i in range(5):\n",
    "    encoder_output1, encoder_state1 = lstm_cell(hs1[i], state1)\n",
    "# decoder\n",
    "decoder_state1 = encoder_state1\n",
    "output1 = []\n",
    "for i in range(5):\n",
    "    decoder_output1, decoder_state1 = lstm_cell(hs1[i], decoder_state1)\n",
    "    output1.append(tf.contrib.layers.fully_connected(decoder_output1, target_size, tf.tanh))\n",
    "\n",
    "logits_flat1 = tf.stack(output1, axis=1)\n",
    "logits1 = tf.reshape(logits_flat1, [-1, 5, target_size]) \n",
    "loss1 = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits1, labels=order))\n",
    "\n",
    "# Model2\n",
    "initial_state2 = state2 = lstm_cell.zero_state(batch_size,tf.float32)\n",
    "\n",
    "for i in range(5):\n",
    "    encoder_output2, encoder_state2 = lstm_cell(hs2[i], state2)\n",
    "# decoder\n",
    "decoder_state2 = encoder_state2\n",
    "output2 = []\n",
    "for i in range(5):\n",
    "    decoder_output2, decoder_state2 = lstm_cell(hs2[i], decoder_state2)\n",
    "    output2.append(tf.contrib.layers.fully_connected(decoder_output2, target_size,tf.tanh))\n",
    "\n",
    "logits_flat2 = tf.stack(output2, axis=1)\n",
    "logits2 = tf.reshape(logits_flat2, [-1, 5, target_size]) \n",
    "loss2 = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits2, labels=order_reverse))\n",
    "\n",
    "# Predict\n",
    "unpacked_logits1 = [tensor for tensor in tf.unstack(logits1, axis=1)]\n",
    "unpacked_logits2 = [tensor for tensor in tf.unstack(logits2, axis=1)]\n",
    "combine_logits = [ unpacked_logits2[-1], unpacked_logits2[-2], unpacked_logits1[-3], unpacked_logits1[-2], unpacked_logits1[-1]]\n",
    "softmaxes = [tf.nn.softmax(tensor) for tensor in combine_logits]\n",
    "softmaxed_logits = tf.stack(softmaxes, axis=1)\n",
    "predict = tf.argmax(softmaxed_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_op1 = tf.train.AdamOptimizer(0.001).minimize(loss1)\n",
    "opt_op2 = tf.train.AdamOptimizer(0.001).minimize(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 0 -----\n",
      " Model1 Train loss: 6.22957585916\n",
      " Model2 Train loss: 6.22701385828\n",
      " Dev accuracy: 0.518439337253\n",
      "----- Epoch 1 -----\n",
      " Model1 Train loss: 5.41817140783\n",
      " Model2 Train loss: 5.41402065838\n",
      " Dev accuracy: 0.534901122394\n",
      "----- Epoch 2 -----\n",
      " Model1 Train loss: 5.24858183771\n",
      " Model2 Train loss: 5.24140199178\n",
      " Dev accuracy: 0.52859433458\n",
      "----- Epoch 3 -----\n",
      " Model1 Train loss: 5.1038672693\n",
      " Model2 Train loss: 5.10497755562\n",
      " Dev accuracy: 0.53115980759\n",
      "----- Epoch 4 -----\n",
      " Model1 Train loss: 4.95824420988\n",
      " Model2 Train loss: 4.94460008387\n",
      " Dev accuracy: 0.537359700695\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "BATCH_SIZE = 25\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    n = train_stories.shape[0]\n",
    "\n",
    "    for epoch in range(5):\n",
    "        print('----- Epoch', epoch, '-----')\n",
    "        total_loss1 = 0\n",
    "        total_loss2 = 0\n",
    "        for i in range(n // BATCH_SIZE):\n",
    "            batch_list = random.sample(range(len(train_stories)),BATCH_SIZE)\n",
    "            inst_story = train_stories[batch_list]\n",
    "            inst_order = train_orders[batch_list]\n",
    "            inst_order_reverse = train_orders_reverse[batch_list]\n",
    "            feed_dict = {story: inst_story, order: inst_order,order_reverse:inst_order_reverse}\n",
    "            _,_, current_loss1, current_loss2 = sess.run([opt_op1,opt_op2, loss1,loss2], feed_dict=feed_dict)\n",
    "            total_loss1 += current_loss1\n",
    "            total_loss2 += current_loss2\n",
    "\n",
    "        print(' Model1 Train loss:', total_loss1 / n)\n",
    "        print(' Model2 Train loss:', total_loss2 / n)\n",
    "        '''\n",
    "        train_feed_dict = {story: train_stories, order: train_orders}\n",
    "        train_predicted = sess.run(predict, feed_dict=train_feed_dict)\n",
    "        train_accuracy = nn.calculate_accuracy(train_orders, train_predicted)\n",
    "        print(' Train accuracy:', train_accuracy)\n",
    "        '''\n",
    "        dev_feed_dict = {story: dev_stories, order: dev_orders,order_reverse:dev_order_reverse}\n",
    "        dev_predicted = sess.run(predict, feed_dict=dev_feed_dict)\n",
    "        dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predicted)\n",
    "        print(' Dev accuracy:', dev_accuracy)\n",
    "\n",
    "        \n",
    "    \n",
    "    nn.save_model(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
