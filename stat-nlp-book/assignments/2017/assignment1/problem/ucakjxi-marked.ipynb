{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **>>> You were told not to change the setup and/or assessment cells, yet you did so. The magnitude of change doesn't matter. Next time you'll lose points for that. <<<**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "In this assignment you will build a language model for the [OHHLA corpus](http://ohhla.com/) we are using in the book. You will train the model on the available training set, and can tune it on the development set. After submission we will run your notebook on a different test set. Your mark will depend on \n",
    "\n",
    "* whether your language model is **properly normalized**,\n",
    "* its **perplexity** on the unseen test set,\n",
    "* your **description** of your approach. \n",
    "\n",
    "To develop your model you have access to:\n",
    "\n",
    "* The training and development data in `data/ohhla`.\n",
    "* The code of the lecture, stored in a python module [here](/edit/statnlpbook/lm.py).\n",
    "* Libraries on the [docker image](https://github.com/uclmr/stat-nlp-book/blob/python/Dockerfile) which contains everything in [this image](https://github.com/jupyter/docker-stacks/tree/master/scipy-notebook), including scikit-learn and tensorflow. \n",
    "\n",
    "As we have to run the notebooks of all students, and because writing efficient code is important, **your notebook should run in 5 minutes at most**, on your machine. Further comments:\n",
    "\n",
    "* We have tested a possible solution on the Azure VMs and it ran in seconds, so it is possible to train a reasonable LM on the data in reasonable time. \n",
    "\n",
    "* Try to run your parameter optimisation offline, such that in your answer notebook the best parameters are already set and don't need to be searched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "It is important that this file is placed in the **correct directory**. It will not run otherwise. The correct directory is\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2017/assignment1/problem/\n",
    "    \n",
    "where `DIRECTORY_OF_YOUR_BOOK` is a placeholder for the directory you downloaded the book to. After you placed it there, **rename the file** to your UCL ID (of the form `ucxxxxx`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Instructions\n",
    "This notebook will be used by you to provide your solution, and by us to both assess your solution and enter your marks. It contains three types of sections:\n",
    "\n",
    "1. **Setup** Sections: these sections set up code and resources for assessment. **Do not edit these**. \n",
    "2. **Assessment** Sections: these sections are used for both evaluating the output of your code, and for markers to enter their marks. **Do not edit these**. \n",
    "3. **Task** Sections: these sections require your solutions. They may contain stub code, and you are expected to edit this code. For free text answers simply edit the markdown field.  \n",
    "\n",
    "Note that you are free to **create additional notebook cells** within a task section. \n",
    "\n",
    "Please **do not share** this assignment publicly, by uploading it online, emailing it to friends etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "To submit your solution:\n",
    "\n",
    "* Make sure that your solution is fully contained in this notebook. \n",
    "* **Rename this notebook to your UCL ID** (of the form \"ucxxxxx\"), if you have not already done so.\n",
    "* Download the notebook in Jupyter via *File -> Download as -> Notebook (.ipynb)*.\n",
    "* Upload the notebook to the Moodle submission site.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries\n",
    "This cell loads libraries important for evaluation and assessment of your model. **Do not change it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! SETUP 1\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../\"\n",
    "sys.path.append(_snlp_book_dir) \n",
    "import statnlpbook.lm as lm\n",
    "import statnlpbook.ohhla as ohhla\n",
    "import math\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Load Training Data\n",
    "\n",
    "This cell loads the training data. We use this data for assessment to define the reference vocabulary: the union of the words of the training and set set. You can use the dataset to train your model, but you are also free to load the data in a different way, or focus on subsets etc. However, when you do this, still **do not edit this setup section**. Instead refer to the variables in your own code, and slice and dice them as you see fit.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load ../../../..//data/ohhla/train/www.ohhla.com/anonymous/nas/distant/tribal.nas.txt.html\n"
     ]
    }
   ],
   "source": [
    "#! SETUP 2\n",
    "_snlp_train_dir = _snlp_book_dir + \"/data/ohhla/train\"\n",
    "_snlp_dev_dir = _snlp_book_dir + \"/data/ohhla/dev\"\n",
    "_snlp_train_song_words = ohhla.words(ohhla.load_all_songs(_snlp_train_dir))\n",
    "_snlp_dev_song_words = ohhla.words(ohhla.load_all_songs(_snlp_dev_dir))\n",
    "assert(len(_snlp_train_song_words)==1041496)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to file encoding issues this code produces one error `Could not load ...`. **Ignore this error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1</font>: Develop and Train the Model\n",
    "\n",
    "This is the core part of the assignment. You are to code up, train and tune a language model. Your language model needs to be subclass of the `lm.LanguageModel` class. You can use some of the existing language models developed in the lecture, or develop your own extensions. \n",
    "\n",
    "Concretely, you need to return a better language model in the `create_lm` function. This function receives a target vocabulary `vocab`, and it needs to return a language model defined over this vocabulary. \n",
    "\n",
    "The target vocab will be the union of the training and test set (hidden to you at development time). This vocab will contain words not in the training set. One way to address this issue is to use the `lm.OOVAwareLM` class discussed in the lecture notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the code for N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class NGramLM(lm.CountLM):\n",
    "    def __init__(self, train, order):\n",
    "        \"\"\"\n",
    "        Create an NGram language model.\n",
    "        Args:\n",
    "            train: list of training tokens.\n",
    "            order: order of the LM.\n",
    "        \"\"\"\n",
    "        super().__init__(set(train), order)\n",
    "        self._counts = collections.defaultdict(float)\n",
    "        self._norm = collections.defaultdict(float)\n",
    "        self._norm_counts = collections.defaultdict(float)\n",
    "        seen = set()\n",
    "        for i in range(self.order, len(train)):\n",
    "            history = tuple(train[i - self.order + 1: i])\n",
    "            word = train[i]\n",
    "            self._counts[(word,) + history] += 1.0\n",
    "            self._norm[history] += 1.0\n",
    "#             print (\" \".join((word,) + history))\n",
    "            if \"\".join((word,) + history) not in seen:\n",
    "                self._norm_counts[history] += 1.0\n",
    "                seen.add(\"\".join((word,) + history))\n",
    "\n",
    "    def counts(self, word_and_history):\n",
    "        return self._counts[word_and_history]\n",
    "\n",
    "    def norm(self, history):\n",
    "        return self._norm[history]\n",
    "    \n",
    "class KneserNey(lm.CountLM):\n",
    "    def __init__(self, train, order):\n",
    "        super().__init__(set(train), order)\n",
    "        self._counts = collections.defaultdict(float)\n",
    "        self._norm = collections.defaultdict(float)\n",
    "        self._word_counts = collections.defaultdict(float)\n",
    "        self.total_bigrams = 0\n",
    "        bigram_seen = set()\n",
    "        word_seen = set()\n",
    "        for i in range(self.order, len(train)):\n",
    "            history = tuple(train[i - self.order: i])\n",
    "            word = train[i]\n",
    "            self._counts[(word,) + history] += 1.0\n",
    "            self._norm[history] += 1.0\n",
    "#             print (\" \".join((word,) + history))\n",
    "            if \"\".join((word,) + history) not in word_seen:\n",
    "                self._word_counts[word] += 1.0\n",
    "                word_seen.add(\"\".join((word,) + history))\n",
    "            if \"\".join((word,) + history) not in bigram_seen:\n",
    "                self.total_bigrams += 1\n",
    "                bigram_seen.add(\"\".join((word,) + history))\n",
    "                \n",
    "    def probability(self, word, *history):\n",
    "        # word = word.lower()\n",
    "        if word not in self.vocab:\n",
    "            return 0.0\n",
    "        return self._word_counts[word] / self.total_bigrams\n",
    "\n",
    "    def counts(self, word_and_history):\n",
    "        return self._counts[word_and_history]\n",
    "\n",
    "    def norm(self, history):\n",
    "        return self._norm[history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsoluteDiscounting(lm.LanguageModel):\n",
    "    def __init__(self, main, backoff, d,missing_words):\n",
    "        super().__init__(main.vocab, main.order)\n",
    "        self.main = main\n",
    "        self.backoff = backoff\n",
    "        self.d = d\n",
    "        self.missing_words = missing_words\n",
    "        \n",
    "    def probability(self, word, *history):\n",
    "        is_oov = False\n",
    "        if word in self.main.vocab:\n",
    "            word = word\n",
    "            is_oov = False\n",
    "        elif word in self.missing_words:\n",
    "            word = lm.OOV\n",
    "            is_oov = True\n",
    "\n",
    "        sub_history = tuple(history[-(self.order - 1):]) if self.order > 1 else ()        \n",
    "        if self.main.norm(sub_history) > 0 and self.main._norm_counts[sub_history] > 0:\n",
    "            a = max( self.main.counts((word,) + sub_history) - self.d ,0 )/self.main.norm(sub_history) \n",
    "            normalizer = ( self.d / self.main.norm(sub_history)  ) * self.main._norm_counts[sub_history]            \n",
    "            b = normalizer * self.backoff.probability(word, *history) \n",
    "            if is_oov:\n",
    "                return (a + b)/len(self.missing_words)\n",
    "            return a + b\n",
    "        else:\n",
    "            if is_oov:\n",
    "                return self.backoff.probability(word, *history)/len(self.missing_words)\n",
    "            return self.backoff.probability(word, *history) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You should improve this cell\n",
    "def create_lm(vocab):\n",
    "    \"\"\"\n",
    "    Return an instance of `lm.LanguageModel` defined over the given vocabulary.\n",
    "    Args:\n",
    "        vocab: the vocabulary the LM should be defined over. It is the union of the training and test words.\n",
    "    Returns:\n",
    "        a language model, instance of `lm.LanguageModel`.\n",
    "    \"\"\"\n",
    "    train_corpus = lm.inject_OOVs(_snlp_train_song_words)\n",
    "    _snlp_train_vocab = set(train_corpus)\n",
    "    missing_words = vocab - _snlp_train_vocab \n",
    "    \n",
    "    lm1 = NGramLM(train_corpus,2)\n",
    "    lm2 = KneserNey(train_corpus,1)\n",
    "    lm3 = NGramLM(_snlp_train_song_words,3)\n",
    "    lm4 = NGramLM(_snlp_train_song_words,4)\n",
    "    lm5 = NGramLM(_snlp_train_song_words,5)\n",
    "    lm6 = NGramLM(_snlp_train_song_words,6)\n",
    "    lm7 = NGramLM(_snlp_train_song_words,7)\n",
    "    lm8 = NGramLM(_snlp_train_song_words,8)\n",
    "    lm9 = NGramLM(_snlp_train_song_words,9)\n",
    "\n",
    "    full_lm = AbsoluteDiscounting(lm1,lm2,0.9,missing_words)\n",
    "    full_lm = AbsoluteDiscounting(lm3,full_lm,0.95,missing_words)\n",
    "    full_lm = AbsoluteDiscounting(lm4,full_lm,0.95,missing_words)\n",
    "    full_lm = AbsoluteDiscounting(lm5,full_lm,0.75,missing_words)\n",
    "    full_lm = AbsoluteDiscounting(lm6,full_lm,0.6,missing_words)\n",
    "    full_lm = AbsoluteDiscounting(lm7,full_lm,0.5,missing_words)\n",
    "    full_lm = AbsoluteDiscounting(lm8,full_lm,0.45,missing_words)\n",
    "    full_lm = AbsoluteDiscounting(lm9,full_lm,0.55,missing_words)\n",
    "\n",
    "    return full_lm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the code for LSTM.\n",
    "Just a try, not using for create_lm() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def word2id(_snlp_train_vocab):\n",
    "    return dict(zip(_snlp_train_vocab,range(len(_snlp_train_vocab)))),dict(zip(range(len(_snlp_train_vocab)),_snlp_train_vocab))\n",
    "\n",
    "class LSTMModel():\n",
    "    def __init__(self,train_set,dev_set,order,vocab,batch_size,is_preprocess,embedding_size=50,lstm_hidden_size=50,hidden_size=50,learning_rate=1e-4):\n",
    "        self.train_vocab,count = lm.inject_OOVs(train_set)\n",
    "        \n",
    "        self.train_set = self.train_vocab\n",
    "        self.dev_set = dev_set\n",
    "        self.order = order\n",
    "        self.vocab = set(self.train_set)\n",
    "        \n",
    "        self._snlp_train_vocab = set(self.train_set)\n",
    "        self.vocab_size = len(self._snlp_train_vocab)\n",
    "        self.missing_words = vocab - self._snlp_train_vocab\n",
    "        print (self.vocab_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.w2id,self.id2w = word2id(self._snlp_train_vocab)\n",
    "        self.num_batchs = int(len(train_set)/self.batch_size)\n",
    "\n",
    "        if is_preprocess:\n",
    "            train_set = [self.w2id[word] for word in self.train_set]\n",
    "            def get_batch():\n",
    "                x_batch = np.zeros([self.batch_size,self.order-1],dtype=np.int32)\n",
    "                y_batch = np.zeros([self.batch_size],dtype=np.int32)\n",
    "                for i in range(self.batch_size):\n",
    "                    index = np.random.randint(self.order+1,len(train_set))\n",
    "                    y_batch[i] = train_set[index]\n",
    "                    x_batch[i,:] = train_set[index-self.order+1:index]\n",
    "                return x_batch,y_batch\n",
    "\n",
    "            self.batchs = []\n",
    "            for i in range(int(len(train_set)/self.batch_size)):\n",
    "                x_batch,y_batch = get_batch()\n",
    "                self.batchs.append([x_batch,y_batch])\n",
    "            \n",
    "            file = open(\"batchs.pkl\",\"wb\")\n",
    "            pickle.dump(self.batchs,file, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "            file.close()\n",
    "            print (\"batchs loaded: \",len(train_set),\"batches\")\n",
    "        else:\n",
    "            file = open(\"batchs.pkl\",\"rb\")\n",
    "            self.batchs = pickle.load(file) \n",
    "            file.close()\n",
    "            print (\"batchs loaded\")\n",
    "        \n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.x = tf.placeholder(dtype=tf.int32)\n",
    "                self.y = tf.placeholder(dtype=tf.int32)\n",
    "                self.keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "                \n",
    "            with tf.device('/cpu:0'):\n",
    "                self.embedding = tf.get_variable(\n",
    "                  \"embedding\", [self.vocab_size, embedding_size])\n",
    "\n",
    "                inputs = tf.nn.embedding_lookup(self.embedding, self.x)\n",
    "                self.inputs = inputs\n",
    "                \n",
    "            with tf.device('/cpu:0'):\n",
    "                limV = np.sqrt(6. / (embedding_size + lstm_hidden_size * 2))\n",
    "                limG = limV * 4\n",
    "                \n",
    "                def randMatrix(rng, shape, lim):\n",
    "                    return np.asarray(\n",
    "                        rng.uniform(\n",
    "                            low=-lim,\n",
    "                            high=lim,\n",
    "                            size=shape\n",
    "                        ),\n",
    "                        dtype=np.float32\n",
    "                    )\n",
    "                rng = np.random\n",
    "                # Parameters:\n",
    "                # Input gate: input, previous output, and bias.\n",
    "                ix_value = randMatrix(rng, (embedding_size, lstm_hidden_size), limG)\n",
    "                ix = tf.Variable(ix_value)\n",
    "                im_value = randMatrix(rng, (lstm_hidden_size, lstm_hidden_size), limG)\n",
    "                im = tf.Variable(im_value)\n",
    "                ib = tf.Variable(tf.zeros([1, lstm_hidden_size]))\n",
    "                # Forget gate: input, previous output, and bias.\n",
    "                fx_value = randMatrix(rng, (embedding_size, lstm_hidden_size), limG)\n",
    "                fx = tf.Variable(fx_value)\n",
    "                fm_value = randMatrix(rng, (lstm_hidden_size, lstm_hidden_size), limG)\n",
    "                fm = tf.Variable(fm_value)\n",
    "                fb = tf.Variable(tf.zeros([1, lstm_hidden_size]))\n",
    "                # Memory cell: input, state and bias.    \n",
    "                cx_value = randMatrix(rng, (embedding_size, lstm_hidden_size), limV)\n",
    "                cx = tf.Variable(cx_value)\n",
    "                cm_value = randMatrix(rng, (lstm_hidden_size, lstm_hidden_size), limV)\n",
    "                cm = tf.Variable(cm_value)\n",
    "                cb = tf.Variable(tf.zeros([1, lstm_hidden_size]))\n",
    "                \n",
    "                # Output gate: input, previous output, and bias.\n",
    "                ox_value = randMatrix(rng, (embedding_size, lstm_hidden_size), limG)\n",
    "                ox = tf.Variable(ox_value)\n",
    "                om_value = randMatrix(rng, (lstm_hidden_size, lstm_hidden_size), limG)\n",
    "                om = tf.Variable(om_value)\n",
    "                ob = tf.Variable(tf.zeros([1, lstm_hidden_size]))\n",
    "\n",
    "                \n",
    "                W_atten_values = np.asarray(\n",
    "                    rng.uniform(\n",
    "                        low=-np.sqrt(6. / (lstm_hidden_size + 100)),\n",
    "                        high=np.sqrt(6. / (lstm_hidden_size + 100)),\n",
    "                        size=(lstm_hidden_size, 100)\n",
    "                    ),\n",
    "                    dtype=np.float32\n",
    "                )\n",
    "                w_atten = tf.Variable(W_atten_values)\n",
    "                b_atten = tf.Variable(tf.zeros([100]))\n",
    "                \n",
    "                v_values = np.asarray(\n",
    "                    rng.normal(scale=0.1, size=(100,)),\n",
    "                    dtype=np.float32\n",
    "                )\n",
    "                v = tf.Variable(v_values)\n",
    "            \n",
    "                # Classifier weights and biases.\n",
    "                \n",
    "                w_1 = tf.Variable(tf.truncated_normal([lstm_hidden_size, hidden_size], -0.1, 0.1))\n",
    "                b_1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "                \n",
    "                w = tf.Variable(tf.truncated_normal([hidden_size, self.vocab_size], -0.1, 0.1))\n",
    "                b = tf.Variable(tf.zeros([self.vocab_size]))\n",
    "\n",
    "                ## LSTM layer\n",
    "                def lstm_cell(i, prev, state):\n",
    "                    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(prev, im) + ib)\n",
    "                    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(prev, fm) + fb)\n",
    "                    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(prev, om) + ob)\n",
    "                    update = tf.tanh(tf.matmul(i, cx) + tf.matmul(prev , cm) + cb)\n",
    "\n",
    "                    CC = forget_gate * state + input_gate * update\n",
    "                    h = output_gate * tf.tanh(CC)\n",
    "                    return h, CC\n",
    "\n",
    "                output, state = tf.zeros_like(tf.matmul(inputs[:,0,:], ix)), tf.zeros_like(tf.matmul(inputs[:,0,:], ix))\n",
    "                outputs = []\n",
    "                for i in range(self.order-1):\n",
    "                    output, state = lstm_cell(inputs[:,i,:], output, state)\n",
    "                    output_3 = tf.expand_dims(output,1)\n",
    "                    outputs.append(output_3)\n",
    "                outputs = tf.concat(outputs,axis=1)\n",
    "                \n",
    "                ## attention layer\n",
    "                outputs = tf.transpose(outputs, perm=[1, 0, 2]) \n",
    "                atten = tf.nn.tanh(tf.matmul(outputs,w_atten) + b_atten)\n",
    "                w_atten = tf.expand_dims(w_atten,0)\n",
    "                w_atten = tf.tile(w_atten,[self.order-1,1,1])\n",
    "                atten = tf.nn.tanh(outputs @ w_atten + b_atten)\n",
    "                atten = tf.reduce_sum(atten * v, axis=2)\n",
    "                atten = tf.nn.softmax(tf.transpose(atten, perm=[1, 0]))\n",
    "                atten = tf.expand_dims(tf.transpose(atten, perm=[1, 0]),2)\n",
    "                lstm_output = tf.reduce_sum(atten * outputs, axis = 0)\n",
    "                \n",
    "                self.lstm_output = lstm_output\n",
    "                \n",
    "                ## fully connected layer\n",
    "                hidden = tf.nn.relu(tf.matmul(lstm_output,w_1) + b_1)\n",
    "                hidden = tf.nn.dropout(hidden,keep_prob = self.keep_prob)\n",
    "                \n",
    "                logits = tf.matmul(hidden,w) + b \n",
    "                self.predicted = tf.nn.softmax(logits=logits)\n",
    "            with tf.device('/cpu:0'):\n",
    "                y_ = tf.one_hot(self.y,depth=self.vocab_size,dtype=tf.int64)\n",
    "            with tf.device('/cpu:0'):\n",
    "                ## cost function\n",
    "                self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits , labels=y_))\n",
    "                self.train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "    def trianing(self,num_iteration):\n",
    "        \n",
    "        val_set = [self.w2id[word] if word in self.vocab else self.w2id[lm.OOV] for word in self.dev_set]\n",
    "        \n",
    "        self.session = tf.InteractiveSession(graph=self.graph)\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        min_cost = 10000000\n",
    "        cost_list = []\n",
    "        \n",
    "        for i in range(num_iteration):\n",
    "            for _ in range(500):\n",
    "                n = np.random.randint(self.num_batchs)\n",
    "                x_batch,y_batch = self.batchs[n]\n",
    "                feed_dict = {self.x:x_batch,self.y:y_batch,self.keep_prob : 0.5}\n",
    "                _, cost = self.session.run([self.train_op, self.cost], feed_dict=feed_dict)\n",
    "                cost_list.append(cost)\n",
    "        \n",
    "        \n",
    "            def get_batch(data_set):\n",
    "                x_batch = np.zeros([self.batch_size,self.order-1],dtype=np.int32)\n",
    "                y_batch = np.zeros([self.batch_size],dtype=np.int32)\n",
    "                for i in range(self.batch_size):\n",
    "                    index = np.random.randint(self.order+1,len(data_set))\n",
    "                    y_batch[i] = data_set[index]\n",
    "                    x_batch[i,:] = data_set[index-self.order+1:index]\n",
    "                return x_batch,y_batch\n",
    "            val_cost_list = []\n",
    "            for _ in range(50):\n",
    "                val_x_batch,val_y_batch = get_batch(val_set)\n",
    "                feed_dict = {self.x:val_x_batch,self.y:val_y_batch,self.keep_prob : 1}\n",
    "                cost = self.session.run(self.cost, feed_dict=feed_dict)\n",
    "                val_cost_list.append(cost)\n",
    "            print (str(i)+\"th\",\"validation set cost: \",np.mean(val_cost_list))\n",
    "        \n",
    "            val_cost = np.mean(val_cost_list)\n",
    "            if val_cost <= min_cost:\n",
    "                save_path = saver.save(self.session, \"./model\")\n",
    "                print (\"model saved\")\n",
    "                min_cost = val_cost\n",
    "        return cost_list\n",
    "    \n",
    "    def loadmodel(self):\n",
    "        saver = tf.train.Saver()\n",
    "        self.test_sess = tf.InteractiveSession(graph=self.graph)\n",
    "        saver.restore(self.test_sess, \"./model\")\n",
    "    \n",
    "    def probability(self,word,*history):\n",
    "        if self.test_sess == None:\n",
    "            print (\"Need to load model first\")\n",
    "        is_missing = False\n",
    "        if word not in self.vocab:\n",
    "            word = lm.OOV\n",
    "            is_missing = True\n",
    "            \n",
    "        history = list(history)\n",
    "        history = [w if w in self.vocab else lm.OOV for w in history ]\n",
    "            \n",
    "        x_batch = np.zeros([1,self.order-1])\n",
    "        x_batch[0,:] = [self.w2id[w] for w in history]\n",
    "        \n",
    "        feed_dict = {self.x:x_batch,self.keep_prob : 1}\n",
    "        predicted = self.test_sess.run(self.predicted,feed_dict=feed_dict)\n",
    "        ans = predicted[0,self.w2id[word]]\n",
    "        if is_missing:\n",
    "            ans = ans/len(self.missing_words)\n",
    "        return ans\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-794d0dfa8af1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mlstm_hidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m650\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m650\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     learning_rate=1e-3)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c0d01479bccc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_set, dev_set, order, vocab, batch_size, is_preprocess, embedding_size, lstm_hidden_size, hidden_size, learning_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_preprocess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_hidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_OOVs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "train_set = _snlp_train_song_words\n",
    "dev_set = _snlp_dev_song_words\n",
    "order = 31\n",
    "vocab = set(_snlp_train_song_words) | set(_snlp_dev_song_words)\n",
    "batch_size = 20\n",
    "lstm_lm = LSTMModel(train_set,\n",
    "                    dev_set,\n",
    "                    order,\n",
    "                    vocab,\n",
    "                    batch_size,\n",
    "                    True,\n",
    "                    embedding_size=200,\n",
    "                    lstm_hidden_size=650,\n",
    "                    hidden_size=650,\n",
    "                    learning_rate=1e-3)\n",
    "                    \n",
    "# model training\n",
    "cost_list = lstm_lm.trianing(100)\n",
    "plt.plot(cost_list)\n",
    "plt.show()\n",
    "\n",
    "# model testing\n",
    "lstm_lm.loadmodel()\n",
    "lm.perplexity(lstm_lm, _snlp_dev_song_words)      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 3</font>: Specify Test Data\n",
    "This cell defines the directory to load the test songs from. Currently, this points to the dev set but when we evaluate your notebook we will point this directory elsewhere and use a **hidden test set**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! SETUP 3\n",
    "_snlp_test_dir = _snlp_book_dir + \"/data/ohhla/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 4</font>: Load Test Data and Prepare Language Model\n",
    "In this section we load the test data, prepare the reference vocabulary and then create your language model based on this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../..//data/ohhla/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7dfc99a8d4af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#! SETUP 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_snlp_test_song_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mohhla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mohhla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_all_songs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_snlp_test_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_snlp_test_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_snlp_test_song_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m_snlp_dev_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_snlp_dev_song_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_snlp_train_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_snlp_train_song_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/statnlpbook/ohhla.py\u001b[0m in \u001b[0;36mload_all_songs\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_all_songs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0monly_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'txt'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0monly_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mlyrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_song\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0monly_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../..//data/ohhla/test'"
     ]
    }
   ],
   "source": [
    "#! SETUP 4\n",
    "_snlp_test_song_words = ohhla.words(ohhla.load_all_songs(_snlp_test_dir))\n",
    "_snlp_test_vocab = set(_snlp_test_song_words)\n",
    "_snlp_dev_vocab = set(_snlp_dev_song_words)\n",
    "_snlp_train_vocab = set(_snlp_train_song_words)\n",
    "_snlp_vocab = _snlp_test_vocab | _snlp_train_vocab | _snlp_dev_vocab\n",
    "_snlp_lm = create_lm(_snlp_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1</font>: Test Normalization (20 pts)\n",
    "Here we test whether the conditional distributions of your language model are properly normalized. If probabilities sum up to $1$ you get full points, you get half of the points if probabilities sum up to be smaller than 1, and 0 points otherwise. Due to floating point issues we will test with respect to a tolerance $\\epsilon$ (`_eps`).\n",
    "\n",
    "Points:\n",
    "* 10 pts: $\\leq 1 + \\epsilon$\n",
    "* 20 pts: $\\approx 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 1.0000000000000626, ~1: True, <=1: True\n",
      "Sum: 1.0000000000004305, ~1: True, <=1: True\n",
      "Sum: 0.999999999999878, ~1: True, <=1: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! ASSESSMENT 1\n",
    "_snlp_test_token_indices = [100, 1000, 10000]\n",
    "_eps = 0.000001\n",
    "approx_1 = []\n",
    "leq_1 = []\n",
    "for i in _snlp_test_token_indices:\n",
    "    result = sum([_snlp_lm.probability(word, *_snlp_test_song_words[i-_snlp_lm.order+1:i]) for word in _snlp_vocab])\n",
    "    approx_1.append(abs(result - 1.0) < _eps)\n",
    "    leq_1.append(result - _eps <= 1.0)\n",
    "    \n",
    "    print(\"Sum: {sum}, ~1: {approx_1}, <=1: {leq_1}\".format(sum=result, \n",
    "                                                            approx_1=abs(result - 1.0) < _eps, \n",
    "                                                            leq_1=result - _eps <= 1.0))\n",
    "(sum(approx_1) == 3, sum(leq_1) == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is marked with **\n",
    "<!-- ASSESSMENT 2: START_POINTS -->\n",
    "20\n",
    "<!-- ASSESSMENT 2: END_POINTS --> \n",
    "points **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Assessment 2</font>: Apply to Test Data (50 pts)\n",
    "\n",
    "We assess how well your LM performs on some unseen test set. Perplexities are mapped to points as follows.\n",
    "\n",
    "* 0-10 pts: uniform perplexity > perplexity > 550, linear\n",
    "* 10-30 pts: 550 > perplexity > 140, linear\n",
    "* 30-50 pts: 140 > perplexity > 105, linear\n",
    "\n",
    "The **linear** mapping maps any perplexity value between the lower and upper bound linearly to a score. For example, if uniform perplexity is $U$ and your model's perplexity is $P\\leq550$, then your score is $10\\frac{P-U}{550-U}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.perplexity(_snlp_lm, _snlp_test_song_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is marked with **\n",
    "<!-- ASSESSMENT 3: START_POINTS -->\n",
    "0\n",
    "<!-- ASSESSMENT 3: END_POINTS --> points**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2</font>: Describe your Approach\n",
    "\n",
    "< Enter a 500 words max description of your model and the way you trained and tuned it here >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 3</font>: Assess Description (30 pts) \n",
    "\n",
    "We will mark the description along the following dimensions: \n",
    "\n",
    "* Clarity (10pts: very clear, 0pts: we can't figure out what you did)\n",
    "* Creativity (10pts: we could not have come up with this, 0pts: Use the unigram model from the lecture notes)\n",
    "* Substance (10pts: implemented complex state-of-the-art LM, 0pts: Use the unigram model from the lecture notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='o'>Introduction</font>:\n",
    "\n",
    "I have developed the language models in two ways:\n",
    " * N-gram models with absolute discounting and Kneser Ney smoothing.\n",
    " * Neural network (LSTM) model built with tensorflow. \n",
    " \n",
    "The perplexity of first method is about 122 on validation set, the perplexity of the second method is about 160 on validation set.\n",
    " \n",
    "## <font color='red'>1. N-gram model</font>:\n",
    "\n",
    "#### Absolute discounting and Kneser Ney\n",
    "The idea of this model is to combine different order N-gram models to obtain better performance than single N-gram. Theoretically, higher order N-gram model leads to more precise probability, but the problem is that along with the order higher of N-gram model, the required data grows exponentially, the small dataset will lead to sparse history record. The way I used to balance the problem is absolute discounting, which combines different order N-gram models using a constant discount $d$ and a normolizer $\\lambda$, the constant discount $d$ is a hyperparameter and the normolizer $\\lambda$ is calculated during producing the probability. AbsoluteDiscounting() and NGramLM() are the implementation of absolute discounting, the reason of rewrite NGramLM() is count first seen history which is used to calculate $\\lambda$ in absolute discounting. To improve the performance of the unigram in absolute discounting, I use Kneser Ney smoothing to calculate the unigram model. The implementation of Kneser Ney is in KneserNey() function which will count the total number of unique bigram pairs and count the number of each word behind any word once. \n",
    "\n",
    "#### Tune process\n",
    "The way to find the how many different order models need to be used and the suitable constant discount $d$ is greedy search. The process of greedy search is first build a unigram and bigram, try every $d$ in a candidates list and calculate corresponding perplexity, find the best $d$ which leads the smallest perplexity. After that, keep $d_{1,2}$ do not change and add trigram into language model and do the same thing to find the best $d_{2,3}$. Iterate these steps until add a new order model leads to bad performance. Then tune each $d_{i,j}$ where $i != j$ and $i=j-1$ while $d_{n,m}$ where $n,m != i,j$ not change. The final language model contains 9 order models from unigram to 9-gram.\n",
    "\n",
    "#### Out of Vocab problem\n",
    "There is possible to receive a word which not in the training set. In order to not return 0, I inject the training set and when facing a OOV word during calculate the probability, replace the word with lm.OOV and divide the probability with the length of missing words in order to norm to 1.\n",
    "\n",
    "\n",
    "## <font color='red'>2. Neural network model</font>:\n",
    "The basic architecture of the neural network language model is a single long short term memory(LSTM) layer with learnable word embedding as input definded in class LSTMModel(). In the forward process, each word of training vocabulary has been embedded into a fixed size vector, initialzed with uniform(6/(f_in+f_out)). The output of the LSTM layer send to attention layer to combine these hidden state. Then using two fully connected layer with dropout and softmax with cross entropy as cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is marked with **\n",
    "<!-- ASSESSMENT 1: START_POINTS -->\n",
    "20\n",
    "<!-- ASSESSMENT 1: END_POINTS --> points**. \n",
    "    \n",
    "Notes:\n",
    "1-9 gram (2-gram uses KN) + absoluyr discounting; greedy grid search for hyperparameter selection; also tried: lstm.\n",
    "Clarity/Creativity/Substance:10/4/6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
